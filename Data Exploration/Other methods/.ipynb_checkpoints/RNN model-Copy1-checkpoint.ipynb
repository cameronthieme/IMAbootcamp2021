{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43a97bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1288267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb10994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF=pd.read_csv(\"../../Data/1999_2021/aggragate_daily_data_2000to2020.csv\",index_col=0)\n",
    "DF_rain=pd.read_csv(\"../../Data/historic_weather.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1469c8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>energy_charge</th>\n",
       "      <th>effluent_flow</th>\n",
       "      <th>influent_flow</th>\n",
       "      <th>volume_used</th>\n",
       "      <th>poured_flow</th>\n",
       "      <th>water_level</th>\n",
       "      <th>energy_generated</th>\n",
       "      <th>energy_stored</th>\n",
       "      <th>maximum_demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>457.1039</td>\n",
       "      <td>226.12</td>\n",
       "      <td>185.42</td>\n",
       "      <td>20.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.08</td>\n",
       "      <td>0.7217</td>\n",
       "      <td>20346.6165</td>\n",
       "      <td>25253.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>465.5526</td>\n",
       "      <td>255.12</td>\n",
       "      <td>459.28</td>\n",
       "      <td>21.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.06</td>\n",
       "      <td>0.8238</td>\n",
       "      <td>20944.1655</td>\n",
       "      <td>26388.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>553.4819</td>\n",
       "      <td>263.12</td>\n",
       "      <td>885.74</td>\n",
       "      <td>23.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.16</td>\n",
       "      <td>0.8334</td>\n",
       "      <td>21915.0000</td>\n",
       "      <td>29633.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>581.4778</td>\n",
       "      <td>373.12</td>\n",
       "      <td>1011.04</td>\n",
       "      <td>25.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.46</td>\n",
       "      <td>1.2159</td>\n",
       "      <td>23301.4890</td>\n",
       "      <td>30230.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>595.3905</td>\n",
       "      <td>483.12</td>\n",
       "      <td>916.95</td>\n",
       "      <td>27.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.76</td>\n",
       "      <td>1.5561</td>\n",
       "      <td>24564.5235</td>\n",
       "      <td>30661.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2000-01-06</td>\n",
       "      <td>608.0315</td>\n",
       "      <td>408.12</td>\n",
       "      <td>915.51</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.96</td>\n",
       "      <td>1.3913</td>\n",
       "      <td>25612.0605</td>\n",
       "      <td>31290.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000-01-07</td>\n",
       "      <td>609.8446</td>\n",
       "      <td>382.12</td>\n",
       "      <td>921.31</td>\n",
       "      <td>30.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>444.19</td>\n",
       "      <td>1.3220</td>\n",
       "      <td>26503.2705</td>\n",
       "      <td>30742.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000-01-08</td>\n",
       "      <td>548.7882</td>\n",
       "      <td>396.12</td>\n",
       "      <td>968.39</td>\n",
       "      <td>32.68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>444.43</td>\n",
       "      <td>1.3761</td>\n",
       "      <td>27489.4455</td>\n",
       "      <td>30555.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2000-01-09</td>\n",
       "      <td>500.3805</td>\n",
       "      <td>252.12</td>\n",
       "      <td>1238.09</td>\n",
       "      <td>36.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>444.68</td>\n",
       "      <td>0.9007</td>\n",
       "      <td>28263.0450</td>\n",
       "      <td>28498.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2000-01-10</td>\n",
       "      <td>610.5973</td>\n",
       "      <td>427.12</td>\n",
       "      <td>810.87</td>\n",
       "      <td>37.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>445.10</td>\n",
       "      <td>1.5339</td>\n",
       "      <td>28823.3385</td>\n",
       "      <td>31625.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2000-01-11</td>\n",
       "      <td>633.0189</td>\n",
       "      <td>452.12</td>\n",
       "      <td>742.89</td>\n",
       "      <td>38.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>445.26</td>\n",
       "      <td>1.6378</td>\n",
       "      <td>29096.5455</td>\n",
       "      <td>31716.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2000-01-12</td>\n",
       "      <td>638.5056</td>\n",
       "      <td>495.12</td>\n",
       "      <td>495.12</td>\n",
       "      <td>38.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>445.38</td>\n",
       "      <td>1.7674</td>\n",
       "      <td>29358.0645</td>\n",
       "      <td>31593.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2000-01-13</td>\n",
       "      <td>641.5576</td>\n",
       "      <td>528.12</td>\n",
       "      <td>601.21</td>\n",
       "      <td>38.51</td>\n",
       "      <td>10.0</td>\n",
       "      <td>445.38</td>\n",
       "      <td>1.8637</td>\n",
       "      <td>29546.5335</td>\n",
       "      <td>31833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2000-01-14</td>\n",
       "      <td>648.6482</td>\n",
       "      <td>347.12</td>\n",
       "      <td>641.08</td>\n",
       "      <td>39.49</td>\n",
       "      <td>23.0</td>\n",
       "      <td>445.41</td>\n",
       "      <td>1.1925</td>\n",
       "      <td>29629.8105</td>\n",
       "      <td>31384.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2000-01-15</td>\n",
       "      <td>589.5792</td>\n",
       "      <td>495.12</td>\n",
       "      <td>544.36</td>\n",
       "      <td>39.66</td>\n",
       "      <td>14.0</td>\n",
       "      <td>445.53</td>\n",
       "      <td>1.7370</td>\n",
       "      <td>29846.0385</td>\n",
       "      <td>30506.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  energy_charge  effluent_flow  influent_flow  volume_used  \\\n",
       "1   2000-01-01       457.1039         226.12         185.42        20.84   \n",
       "2   2000-01-02       465.5526         255.12         459.28        21.53   \n",
       "3   2000-01-03       553.4819         263.12         885.74        23.63   \n",
       "4   2000-01-04       581.4778         373.12        1011.04        25.78   \n",
       "5   2000-01-05       595.3905         483.12         916.95        27.24   \n",
       "6   2000-01-06       608.0315         408.12         915.51        28.95   \n",
       "7   2000-01-07       609.8446         382.12         921.31        30.76   \n",
       "8   2000-01-08       548.7882         396.12         968.39        32.68   \n",
       "9   2000-01-09       500.3805         252.12        1238.09        36.00   \n",
       "10  2000-01-10       610.5973         427.12         810.87        37.28   \n",
       "11  2000-01-11       633.0189         452.12         742.89        38.26   \n",
       "12  2000-01-12       638.5056         495.12         495.12        38.26   \n",
       "13  2000-01-13       641.5576         528.12         601.21        38.51   \n",
       "14  2000-01-14       648.6482         347.12         641.08        39.49   \n",
       "15  2000-01-15       589.5792         495.12         544.36        39.66   \n",
       "\n",
       "    poured_flow  water_level  energy_generated  energy_stored  maximum_demand  \n",
       "1           0.0       443.08            0.7217     20346.6165         25253.0  \n",
       "2           0.0       443.06            0.8238     20944.1655         26388.0  \n",
       "3           0.0       443.16            0.8334     21915.0000         29633.0  \n",
       "4           0.0       443.46            1.2159     23301.4890         30230.0  \n",
       "5           0.0       443.76            1.5561     24564.5235         30661.0  \n",
       "6           0.0       443.96            1.3913     25612.0605         31290.0  \n",
       "7           0.0       444.19            1.3220     26503.2705         30742.0  \n",
       "8           0.0       444.43            1.3761     27489.4455         30555.0  \n",
       "9           0.0       444.68            0.9007     28263.0450         28498.0  \n",
       "10          0.0       445.10            1.5339     28823.3385         31625.0  \n",
       "11          0.0       445.26            1.6378     29096.5455         31716.0  \n",
       "12          0.0       445.38            1.7674     29358.0645         31593.0  \n",
       "13         10.0       445.38            1.8637     29546.5335         31833.0  \n",
       "14         23.0       445.41            1.1925     29629.8105         31384.0  \n",
       "15         14.0       445.53            1.7370     29846.0385         30506.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972b0734",
   "metadata": {},
   "source": [
    "We will look at a simple RNN model to find a relation between water level and rain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "207eb376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>energy_charge</th>\n",
       "      <th>effluent_flow</th>\n",
       "      <th>influent_flow</th>\n",
       "      <th>volume_used</th>\n",
       "      <th>poured_flow</th>\n",
       "      <th>water_level</th>\n",
       "      <th>energy_generated</th>\n",
       "      <th>energy_stored</th>\n",
       "      <th>maximum_demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7303.000000</td>\n",
       "      <td>7303.000000</td>\n",
       "      <td>7303.000000</td>\n",
       "      <td>7303.000000</td>\n",
       "      <td>7303.000000</td>\n",
       "      <td>7303.000000</td>\n",
       "      <td>7303.000000</td>\n",
       "      <td>7303.000000</td>\n",
       "      <td>7303.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>761.050363</td>\n",
       "      <td>381.621946</td>\n",
       "      <td>382.973679</td>\n",
       "      <td>76.350307</td>\n",
       "      <td>41.320702</td>\n",
       "      <td>449.354220</td>\n",
       "      <td>1.494725</td>\n",
       "      <td>73455.053713</td>\n",
       "      <td>38328.306482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>126.663031</td>\n",
       "      <td>255.239604</td>\n",
       "      <td>313.443189</td>\n",
       "      <td>15.452552</td>\n",
       "      <td>152.686579</td>\n",
       "      <td>1.479972</td>\n",
       "      <td>0.649623</td>\n",
       "      <td>27714.395965</td>\n",
       "      <td>5295.560131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>381.081260</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.900000</td>\n",
       "      <td>20.840000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>443.060000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20346.616500</td>\n",
       "      <td>20938.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>667.442940</td>\n",
       "      <td>241.000000</td>\n",
       "      <td>191.920000</td>\n",
       "      <td>62.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>448.100000</td>\n",
       "      <td>1.014822</td>\n",
       "      <td>51303.060655</td>\n",
       "      <td>34636.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>764.835851</td>\n",
       "      <td>322.000000</td>\n",
       "      <td>277.310000</td>\n",
       "      <td>78.930000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>449.660000</td>\n",
       "      <td>1.394540</td>\n",
       "      <td>70769.379000</td>\n",
       "      <td>38890.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>855.557605</td>\n",
       "      <td>435.000000</td>\n",
       "      <td>465.990000</td>\n",
       "      <td>89.740000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>450.630000</td>\n",
       "      <td>1.879905</td>\n",
       "      <td>97575.807000</td>\n",
       "      <td>42158.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1120.809714</td>\n",
       "      <td>2844.000000</td>\n",
       "      <td>3606.540000</td>\n",
       "      <td>100.360000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>451.530000</td>\n",
       "      <td>3.251000</td>\n",
       "      <td>131011.522500</td>\n",
       "      <td>54043.180000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       energy_charge  effluent_flow  influent_flow  volume_used  poured_flow  \\\n",
       "count    7303.000000    7303.000000    7303.000000  7303.000000  7303.000000   \n",
       "mean      761.050363     381.621946     382.973679    76.350307    41.320702   \n",
       "std       126.663031     255.239604     313.443189    15.452552   152.686579   \n",
       "min       381.081260      17.000000      18.900000    20.840000     0.000000   \n",
       "25%       667.442940     241.000000     191.920000    62.800000     0.000000   \n",
       "50%       764.835851     322.000000     277.310000    78.930000     0.000000   \n",
       "75%       855.557605     435.000000     465.990000    89.740000    25.000000   \n",
       "max      1120.809714    2844.000000    3606.540000   100.360000  2100.000000   \n",
       "\n",
       "       water_level  energy_generated  energy_stored  maximum_demand  \n",
       "count  7303.000000       7303.000000    7303.000000     7303.000000  \n",
       "mean    449.354220          1.494725   73455.053713    38328.306482  \n",
       "std       1.479972          0.649623   27714.395965     5295.560131  \n",
       "min     443.060000          0.000000   20346.616500    20938.000000  \n",
       "25%     448.100000          1.014822   51303.060655    34636.270000  \n",
       "50%     449.660000          1.394540   70769.379000    38890.140000  \n",
       "75%     450.630000          1.879905   97575.807000    42158.950000  \n",
       "max     451.530000          3.251000  131011.522500    54043.180000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "034ea7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date        datetime64[ns]\n",
       "hour               float64\n",
       "station             object\n",
       "rain_mm            float64\n",
       "temp_max           float64\n",
       "temp_min           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.date=pd.to_datetime(DF.date)\n",
    "DF_rain.date=pd.to_datetime(DF_rain.date)\n",
    "DF_rain.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc1d9250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create some lags\n",
    "def sum2(x):\n",
    "    if all(x.isna()):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return np.sum(x)\n",
    "\n",
    "\n",
    "KK=DF_rain.groupby(['date','station']).agg(rain_mm=('rain_mm',sum2)).reset_index().sort_values(['station','date'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b53ed543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A705 BAURU</th>\n",
       "      <th>A711 SAO CARLOS</th>\n",
       "      <th>A737 IBITINGA</th>\n",
       "      <th>A741 BARRA BONITA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-01-01</th>\n",
       "      <td>40.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-02</th>\n",
       "      <td>48.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-03</th>\n",
       "      <td>18.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-04</th>\n",
       "      <td>1.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-05</th>\n",
       "      <td>29.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-27</th>\n",
       "      <td>51.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-28</th>\n",
       "      <td>29.8</td>\n",
       "      <td>29.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-29</th>\n",
       "      <td>62.4</td>\n",
       "      <td>12.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-30</th>\n",
       "      <td>15.0</td>\n",
       "      <td>24.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31</th>\n",
       "      <td>10.8</td>\n",
       "      <td>27.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5114 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            A705 BAURU  A711 SAO CARLOS  A737 IBITINGA  A741 BARRA BONITA\n",
       "date                                                                     \n",
       "2007-01-01        40.6              NaN            NaN                NaN\n",
       "2007-01-02        48.8              NaN            NaN                NaN\n",
       "2007-01-03        18.2              NaN            NaN                NaN\n",
       "2007-01-04         1.4              NaN            NaN                NaN\n",
       "2007-01-05        29.4              NaN            NaN                NaN\n",
       "...                ...              ...            ...                ...\n",
       "2020-12-27        51.0             12.8            0.0                0.0\n",
       "2020-12-28        29.8             29.4            0.0                0.0\n",
       "2020-12-29        62.4             12.6            0.0                0.0\n",
       "2020-12-30        15.0             24.4            0.0                0.0\n",
       "2020-12-31        10.8             27.4            0.0                0.0\n",
       "\n",
       "[5114 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rain_pivot=KK.pivot(index='date', columns='station', values='rain_mm')\n",
    "Rain_pivot.columns.name=None\n",
    "Rain_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "399c6856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>energy_charge</th>\n",
       "      <th>effluent_flow</th>\n",
       "      <th>influent_flow</th>\n",
       "      <th>volume_used</th>\n",
       "      <th>poured_flow</th>\n",
       "      <th>water_level</th>\n",
       "      <th>energy_generated</th>\n",
       "      <th>energy_stored</th>\n",
       "      <th>maximum_demand</th>\n",
       "      <th>A705 BAURU</th>\n",
       "      <th>A711 SAO CARLOS</th>\n",
       "      <th>A737 IBITINGA</th>\n",
       "      <th>A741 BARRA BONITA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-01</th>\n",
       "      <td>457.103900</td>\n",
       "      <td>226.12</td>\n",
       "      <td>185.42</td>\n",
       "      <td>20.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.08</td>\n",
       "      <td>0.721700</td>\n",
       "      <td>20346.61650</td>\n",
       "      <td>25253.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-02</th>\n",
       "      <td>465.552600</td>\n",
       "      <td>255.12</td>\n",
       "      <td>459.28</td>\n",
       "      <td>21.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.06</td>\n",
       "      <td>0.823800</td>\n",
       "      <td>20944.16550</td>\n",
       "      <td>26388.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>553.481900</td>\n",
       "      <td>263.12</td>\n",
       "      <td>885.74</td>\n",
       "      <td>23.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.16</td>\n",
       "      <td>0.833400</td>\n",
       "      <td>21915.00000</td>\n",
       "      <td>29633.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>581.477800</td>\n",
       "      <td>373.12</td>\n",
       "      <td>1011.04</td>\n",
       "      <td>25.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.46</td>\n",
       "      <td>1.215900</td>\n",
       "      <td>23301.48900</td>\n",
       "      <td>30230.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>595.390500</td>\n",
       "      <td>483.12</td>\n",
       "      <td>916.95</td>\n",
       "      <td>27.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.76</td>\n",
       "      <td>1.556100</td>\n",
       "      <td>24564.52350</td>\n",
       "      <td>30661.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>912.189387</td>\n",
       "      <td>323.00</td>\n",
       "      <td>323.00</td>\n",
       "      <td>53.89</td>\n",
       "      <td>25.0</td>\n",
       "      <td>447.17</td>\n",
       "      <td>1.163433</td>\n",
       "      <td>30146.37673</td>\n",
       "      <td>43231.117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-28</th>\n",
       "      <td>880.473677</td>\n",
       "      <td>375.00</td>\n",
       "      <td>209.72</td>\n",
       "      <td>53.34</td>\n",
       "      <td>25.0</td>\n",
       "      <td>447.11</td>\n",
       "      <td>1.362688</td>\n",
       "      <td>30230.79799</td>\n",
       "      <td>43006.734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-29</th>\n",
       "      <td>827.558858</td>\n",
       "      <td>299.00</td>\n",
       "      <td>216.71</td>\n",
       "      <td>53.06</td>\n",
       "      <td>25.0</td>\n",
       "      <td>447.08</td>\n",
       "      <td>1.058915</td>\n",
       "      <td>30351.00804</td>\n",
       "      <td>40620.258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>893.432298</td>\n",
       "      <td>345.00</td>\n",
       "      <td>208.08</td>\n",
       "      <td>52.60</td>\n",
       "      <td>25.0</td>\n",
       "      <td>447.03</td>\n",
       "      <td>1.256731</td>\n",
       "      <td>30155.61356</td>\n",
       "      <td>41780.441</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>850.271015</td>\n",
       "      <td>148.00</td>\n",
       "      <td>175.31</td>\n",
       "      <td>52.69</td>\n",
       "      <td>25.0</td>\n",
       "      <td>447.04</td>\n",
       "      <td>0.505311</td>\n",
       "      <td>30023.23611</td>\n",
       "      <td>42182.234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7303 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            energy_charge  effluent_flow  influent_flow  volume_used  \\\n",
       "date                                                                   \n",
       "2000-01-01     457.103900         226.12         185.42        20.84   \n",
       "2000-01-02     465.552600         255.12         459.28        21.53   \n",
       "2000-01-03     553.481900         263.12         885.74        23.63   \n",
       "2000-01-04     581.477800         373.12        1011.04        25.78   \n",
       "2000-01-05     595.390500         483.12         916.95        27.24   \n",
       "...                   ...            ...            ...          ...   \n",
       "2019-12-27     912.189387         323.00         323.00        53.89   \n",
       "2019-12-28     880.473677         375.00         209.72        53.34   \n",
       "2019-12-29     827.558858         299.00         216.71        53.06   \n",
       "2019-12-30     893.432298         345.00         208.08        52.60   \n",
       "2019-12-31     850.271015         148.00         175.31        52.69   \n",
       "\n",
       "            poured_flow  water_level  energy_generated  energy_stored  \\\n",
       "date                                                                    \n",
       "2000-01-01          0.0       443.08          0.721700    20346.61650   \n",
       "2000-01-02          0.0       443.06          0.823800    20944.16550   \n",
       "2000-01-03          0.0       443.16          0.833400    21915.00000   \n",
       "2000-01-04          0.0       443.46          1.215900    23301.48900   \n",
       "2000-01-05          0.0       443.76          1.556100    24564.52350   \n",
       "...                 ...          ...               ...            ...   \n",
       "2019-12-27         25.0       447.17          1.163433    30146.37673   \n",
       "2019-12-28         25.0       447.11          1.362688    30230.79799   \n",
       "2019-12-29         25.0       447.08          1.058915    30351.00804   \n",
       "2019-12-30         25.0       447.03          1.256731    30155.61356   \n",
       "2019-12-31         25.0       447.04          0.505311    30023.23611   \n",
       "\n",
       "            maximum_demand  A705 BAURU  A711 SAO CARLOS  A737 IBITINGA  \\\n",
       "date                                                                     \n",
       "2000-01-01       25253.000         NaN              NaN            NaN   \n",
       "2000-01-02       26388.000         NaN              NaN            NaN   \n",
       "2000-01-03       29633.000         NaN              NaN            NaN   \n",
       "2000-01-04       30230.000         NaN              NaN            NaN   \n",
       "2000-01-05       30661.000         NaN              NaN            NaN   \n",
       "...                    ...         ...              ...            ...   \n",
       "2019-12-27       43231.117         0.0              0.0            NaN   \n",
       "2019-12-28       43006.734         0.0              9.0            NaN   \n",
       "2019-12-29       40620.258         0.0              0.0            NaN   \n",
       "2019-12-30       41780.441         0.0              0.0            NaN   \n",
       "2019-12-31       42182.234         0.0              0.8            NaN   \n",
       "\n",
       "            A741 BARRA BONITA  \n",
       "date                           \n",
       "2000-01-01                NaN  \n",
       "2000-01-02                NaN  \n",
       "2000-01-03                NaN  \n",
       "2000-01-04                NaN  \n",
       "2000-01-05                NaN  \n",
       "...                       ...  \n",
       "2019-12-27                NaN  \n",
       "2019-12-28                NaN  \n",
       "2019-12-29                NaN  \n",
       "2019-12-30                NaN  \n",
       "2019-12-31                NaN  \n",
       "\n",
       "[7303 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combine Both\n",
    "DF=DF.merge(Rain_pivot,how='left', on='date')\n",
    "DF=DF.set_index('date')\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77f31f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_shift=0\n",
    "shifts=range(1,max_shift+1)\n",
    "Shifted=[]\n",
    "for shift in shifts:\n",
    "    DF_shifted=DF.shift(periods=shift)\n",
    "    cols=DF_shifted.columns\n",
    "    DF_shifted.columns=[name+'_'+str(shift) for name in cols]\n",
    "    Shifted.append(DF_shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ea84f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for DF_shifted in Shifted:\n",
    "    DF=DF.merge(DF_shifted, on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd0d2c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF=DF.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6e0e509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>energy_charge</th>\n",
       "      <th>effluent_flow</th>\n",
       "      <th>influent_flow</th>\n",
       "      <th>volume_used</th>\n",
       "      <th>poured_flow</th>\n",
       "      <th>water_level</th>\n",
       "      <th>energy_generated</th>\n",
       "      <th>energy_stored</th>\n",
       "      <th>maximum_demand</th>\n",
       "      <th>A705 BAURU</th>\n",
       "      <th>A711 SAO CARLOS</th>\n",
       "      <th>A737 IBITINGA</th>\n",
       "      <th>A741 BARRA BONITA</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>month_day</th>\n",
       "      <th>year_day</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>457.103900</td>\n",
       "      <td>226.12</td>\n",
       "      <td>185.42</td>\n",
       "      <td>20.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.08</td>\n",
       "      <td>0.721700</td>\n",
       "      <td>20346.61650</td>\n",
       "      <td>25253.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>465.552600</td>\n",
       "      <td>255.12</td>\n",
       "      <td>459.28</td>\n",
       "      <td>21.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.06</td>\n",
       "      <td>0.823800</td>\n",
       "      <td>20944.16550</td>\n",
       "      <td>26388.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>553.481900</td>\n",
       "      <td>263.12</td>\n",
       "      <td>885.74</td>\n",
       "      <td>23.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.16</td>\n",
       "      <td>0.833400</td>\n",
       "      <td>21915.00000</td>\n",
       "      <td>29633.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>581.477800</td>\n",
       "      <td>373.12</td>\n",
       "      <td>1011.04</td>\n",
       "      <td>25.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.46</td>\n",
       "      <td>1.215900</td>\n",
       "      <td>23301.48900</td>\n",
       "      <td>30230.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>595.390500</td>\n",
       "      <td>483.12</td>\n",
       "      <td>916.95</td>\n",
       "      <td>27.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.76</td>\n",
       "      <td>1.556100</td>\n",
       "      <td>24564.52350</td>\n",
       "      <td>30661.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7298</th>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>912.189387</td>\n",
       "      <td>323.00</td>\n",
       "      <td>323.00</td>\n",
       "      <td>53.89</td>\n",
       "      <td>25.0</td>\n",
       "      <td>447.17</td>\n",
       "      <td>1.163433</td>\n",
       "      <td>30146.37673</td>\n",
       "      <td>43231.117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>361</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7299</th>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>880.473677</td>\n",
       "      <td>375.00</td>\n",
       "      <td>209.72</td>\n",
       "      <td>53.34</td>\n",
       "      <td>25.0</td>\n",
       "      <td>447.11</td>\n",
       "      <td>1.362688</td>\n",
       "      <td>30230.79799</td>\n",
       "      <td>43006.734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>362</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7300</th>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>827.558858</td>\n",
       "      <td>299.00</td>\n",
       "      <td>216.71</td>\n",
       "      <td>53.06</td>\n",
       "      <td>25.0</td>\n",
       "      <td>447.08</td>\n",
       "      <td>1.058915</td>\n",
       "      <td>30351.00804</td>\n",
       "      <td>40620.258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>363</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7301</th>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>893.432298</td>\n",
       "      <td>345.00</td>\n",
       "      <td>208.08</td>\n",
       "      <td>52.60</td>\n",
       "      <td>25.0</td>\n",
       "      <td>447.03</td>\n",
       "      <td>1.256731</td>\n",
       "      <td>30155.61356</td>\n",
       "      <td>41780.441</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7302</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>850.271015</td>\n",
       "      <td>148.00</td>\n",
       "      <td>175.31</td>\n",
       "      <td>52.69</td>\n",
       "      <td>25.0</td>\n",
       "      <td>447.04</td>\n",
       "      <td>0.505311</td>\n",
       "      <td>30023.23611</td>\n",
       "      <td>42182.234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>365</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7303 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  energy_charge  effluent_flow  influent_flow  volume_used  \\\n",
       "0    2000-01-01     457.103900         226.12         185.42        20.84   \n",
       "1    2000-01-02     465.552600         255.12         459.28        21.53   \n",
       "2    2000-01-03     553.481900         263.12         885.74        23.63   \n",
       "3    2000-01-04     581.477800         373.12        1011.04        25.78   \n",
       "4    2000-01-05     595.390500         483.12         916.95        27.24   \n",
       "...         ...            ...            ...            ...          ...   \n",
       "7298 2019-12-27     912.189387         323.00         323.00        53.89   \n",
       "7299 2019-12-28     880.473677         375.00         209.72        53.34   \n",
       "7300 2019-12-29     827.558858         299.00         216.71        53.06   \n",
       "7301 2019-12-30     893.432298         345.00         208.08        52.60   \n",
       "7302 2019-12-31     850.271015         148.00         175.31        52.69   \n",
       "\n",
       "      poured_flow  water_level  energy_generated  energy_stored  \\\n",
       "0             0.0       443.08          0.721700    20346.61650   \n",
       "1             0.0       443.06          0.823800    20944.16550   \n",
       "2             0.0       443.16          0.833400    21915.00000   \n",
       "3             0.0       443.46          1.215900    23301.48900   \n",
       "4             0.0       443.76          1.556100    24564.52350   \n",
       "...           ...          ...               ...            ...   \n",
       "7298         25.0       447.17          1.163433    30146.37673   \n",
       "7299         25.0       447.11          1.362688    30230.79799   \n",
       "7300         25.0       447.08          1.058915    30351.00804   \n",
       "7301         25.0       447.03          1.256731    30155.61356   \n",
       "7302         25.0       447.04          0.505311    30023.23611   \n",
       "\n",
       "      maximum_demand  A705 BAURU  A711 SAO CARLOS  A737 IBITINGA  \\\n",
       "0          25253.000         NaN              NaN            NaN   \n",
       "1          26388.000         NaN              NaN            NaN   \n",
       "2          29633.000         NaN              NaN            NaN   \n",
       "3          30230.000         NaN              NaN            NaN   \n",
       "4          30661.000         NaN              NaN            NaN   \n",
       "...              ...         ...              ...            ...   \n",
       "7298       43231.117         0.0              0.0            NaN   \n",
       "7299       43006.734         0.0              9.0            NaN   \n",
       "7300       40620.258         0.0              0.0            NaN   \n",
       "7301       41780.441         0.0              0.0            NaN   \n",
       "7302       42182.234         0.0              0.8            NaN   \n",
       "\n",
       "      A741 BARRA BONITA  year  month  month_day  year_day  weekday  \n",
       "0                   NaN  2000      1          1         1        5  \n",
       "1                   NaN  2000      1          2         2        6  \n",
       "2                   NaN  2000      1          3         3        0  \n",
       "3                   NaN  2000      1          4         4        1  \n",
       "4                   NaN  2000      1          5         5        2  \n",
       "...                 ...   ...    ...        ...       ...      ...  \n",
       "7298                NaN  2019     12         27       361        4  \n",
       "7299                NaN  2019     12         28       362        5  \n",
       "7300                NaN  2019     12         29       363        6  \n",
       "7301                NaN  2019     12         30       364        0  \n",
       "7302                NaN  2019     12         31       365        1  \n",
       "\n",
       "[7303 rows x 19 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF['year']=0\n",
    "DF['month']=0\n",
    "DF['month_day']=0\n",
    "DF['year_day']=0\n",
    "DF['weekday']=0\n",
    "\n",
    "for ind in DF.index:\n",
    "    date=DF.date[ind]\n",
    "    date_st=date.timetuple()\n",
    "    DF.loc[ind,'year']=date_st.tm_year\n",
    "    DF.loc[ind,'month']=date_st.tm_mon\n",
    "    DF.loc[ind,'month_day']=date_st.tm_mday\n",
    "    DF.loc[ind,'year_day']=date_st.tm_yday\n",
    "    DF.loc[ind,'weekday']=date_st.tm_wday\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c3ad964",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF=DF.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af891e83",
   "metadata": {},
   "source": [
    "# Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9a041ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_train='2016-01-01'\n",
    "date_validate='2018-01-01'\n",
    "\n",
    "DF=DF[['energy_generated','water_level','month','effluent_flow','maximum_demand','poured_flow','date']]\n",
    "\n",
    "y_var='energy_generated'\n",
    "X_train=np.array(DF.drop(['date',y_var],axis=1)[DF.date<date_train])\n",
    "y_train=np.array(DF[y_var][DF.date<date_train])\n",
    "\n",
    "X_valid=np.array(DF.drop(['date',y_var],axis=1)[(date_train<=DF.date) & (DF.date<date_validate)])\n",
    "y_valid=np.array(DF[y_var][(date_train<=DF.date) & (DF.date<date_validate)])\n",
    "\n",
    "X_test=np.array(DF.drop(['date',y_var],axis=1)[DF.date>=date_validate])\n",
    "y_test=np.array(DF[y_var][DF.date>=date_validate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c4997b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32814263259348403"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Forecast the mean\n",
    "y_pred=np.mean(y_train)\n",
    "np.mean((y_pred-y_valid)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b4b33c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17149029714665912"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Naive forecast using the previous day\n",
    "y_pred=np.array([y_train[-1]]+list(y_valid[:-1]))\n",
    "np.mean((y_pred-y_valid)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "955fc68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6316826677334865"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Naive forecast error using one month ago\n",
    "y_pred=np.array(list(y_train[-30:])+list(y_valid[:-30]))\n",
    "np.mean((y_pred-y_valid)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5025ed",
   "metadata": {},
   "source": [
    "## Regular NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0355f729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b3abb415",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(1000, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(1000, activation='tanh'))\n",
    "model.add(layers.Dense(1000, activation='relu'))\n",
    "model.add(layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c3626f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 1000)              6000      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 1001      \n",
      "=================================================================\n",
      "Total params: 2,009,001\n",
      "Trainable params: 2,009,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4b904fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we compile the network like so\n",
    "model.compile(optimizer='rmsprop',loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d0084ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected DataType for argument 'Tout' not CategoricalDtype(categories=[(0.0265, 0.674], (0.674, 1.318], (1.318, 1.962], (1.962, 2.607], (2.607, 3.251]],\n, ordered=True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mmake_type\u001b[1;34m(v, arg_name)\u001b[0m\n\u001b[0;32m    192\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m     \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py\u001b[0m in \u001b[0;36mas_dtype\u001b[1;34m(type_value)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m   raise TypeError(\"Cannot convert value %r to a TensorFlow DType.\" %\n\u001b[0m\u001b[0;32m    650\u001b[0m                   (type_value,))\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot convert value CategoricalDtype(categories=[(0.0265, 0.674], (0.674, 1.318], (1.318, 1.962], (1.962, 2.607], (2.607, 3.251]],\n, ordered=True) to a TensorFlow DType.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-125deaaee8a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m history = model.fit(X_train,y_train,epochs = 100,\n\u001b[0m\u001b[0;32m      2\u001b[0m                         validation_data=(X_valid,y_valid))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1047\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m       \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[0;32m   1050\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m           \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1105\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1106\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    474\u001b[0m         \"recommend using that to load a Dataset instead.\")\n\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGenericArrayLikeDataAdapter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"batch\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    518\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m     dataset = indices_dataset.map(\n\u001b[0m\u001b[0;32m    521\u001b[0m         grab_batch, num_parallel_calls=dataset_ops.AUTOTUNE)\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[0;32m   1695\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1696\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1697\u001b[1;33m       return ParallelMapDataset(\n\u001b[0m\u001b[0;32m   1698\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1699\u001b[0m           \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   4078\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4079\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4080\u001b[1;33m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[0;32m   4081\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4082\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3369\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3370\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3371\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3373\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2936\u001b[0m       \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mto\u001b[0m \u001b[0mspecialize\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2937\u001b[0m     \"\"\"\n\u001b[1;32m-> 2938\u001b[1;33m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[0;32m   2939\u001b[0m         *args, **kwargs)\n\u001b[0;32m   2940\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2904\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2906\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2907\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2908\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3362\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m   3363\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3364\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3365\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3366\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3297\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3299\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3300\u001b[0m       \u001b[1;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3301\u001b[0m       \u001b[1;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_whitelisted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m   \u001b[1;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mgrab_batch\u001b[1;34m(indices)\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mslice_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0minp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mflat_inputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m       \u001b[0mflat_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscript_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager_py_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_dtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_inp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdynamic_shape_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_inp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\u001b[0m in \u001b[0;36meager_py_func\u001b[1;34m(func, inp, Tout, name)\u001b[0m\n\u001b[0;32m    454\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost_address_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 456\u001b[1;33m       return _internal_py_func(\n\u001b[0m\u001b[0;32m    457\u001b[0m           func=func, inp=inp, Tout=Tout, eager=True, name=name)\n\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\u001b[0m in \u001b[0;36m_internal_py_func\u001b[1;34m(func, inp, Tout, stateful, eager, is_grad_func, name)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0meager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m     result = gen_script_ops.eager_py_func(\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[0mtoken\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\u001b[0m in \u001b[0;36meager_py_func\u001b[1;34m(input, token, Tout, is_async, name)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;34m\"Expected list for 'Tout' argument to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \"'eager_py_func' Op, not %r.\" % Tout)\n\u001b[1;32m---> 63\u001b[1;33m   \u001b[0mTout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Tout\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_t\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mTout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mis_async\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mis_async\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;34m\"Expected list for 'Tout' argument to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \"'eager_py_func' Op, not %r.\" % Tout)\n\u001b[1;32m---> 63\u001b[1;33m   \u001b[0mTout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Tout\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_t\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mTout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mis_async\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mis_async\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mmake_type\u001b[1;34m(v, arg_name)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m     raise TypeError(\"Expected DataType for argument '%s' not %s.\" %\n\u001b[0m\u001b[0;32m    196\u001b[0m                     (arg_name, repr(v)))\n\u001b[0;32m    197\u001b[0m   \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected DataType for argument 'Tout' not CategoricalDtype(categories=[(0.0265, 0.674], (0.674, 1.318], (1.318, 1.962], (1.962, 2.607], (2.607, 3.251]],\n, ordered=True)."
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,epochs = 100,\n",
    "                        validation_data=(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ae711e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3448478751997961"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally check\n",
    "y_pred=model.predict(X_valid)\n",
    "np.mean((y_pred-y_valid)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b5d5d6c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3628474442412001"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=model.predict(X_test)\n",
    "np.mean((y_pred-y_test)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca349daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "0edc1db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3ff061fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_39 (Dense)             (None, 512)               53760     \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 646,817\n",
      "Trainable params: 646,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "fff4371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we compile the network like so\n",
    "model.compile(optimizer='rmsprop',loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "9be6a391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 10938714.0000 - mse: 10938714.0000\n",
      "Epoch 2/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 29488.8027 - mse: 29488.8027\n",
      "Epoch 3/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 3055.8569 - mse: 3055.8569\n",
      "Epoch 4/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 4841.5732 - mse: 4841.5732\n",
      "Epoch 5/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 154.9090 - mse: 154.9090\n",
      "Epoch 6/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 27.8604 - mse: 27.8604\n",
      "Epoch 7/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 2.0118 - mse: 2.0118\n",
      "Epoch 8/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 1.5187 - mse: 1.5187\n",
      "Epoch 9/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 1.1788 - mse: 1.1788\n",
      "Epoch 10/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.9043 - mse: 0.9043\n",
      "Epoch 11/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.6950 - mse: 0.6950\n",
      "Epoch 12/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.5512 - mse: 0.5512\n",
      "Epoch 13/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4696 - mse: 0.4696\n",
      "Epoch 14/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4432 - mse: 0.4432\n",
      "Epoch 15/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4400 - mse: 0.4400\n",
      "Epoch 16/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 17/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 18/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 19/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 20/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 21/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 22/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 23/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 24/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 25/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 26/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 27/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 28/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 29/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 30/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 31/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 32/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 33/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 34/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 35/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 36/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 37/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 38/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 39/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 40/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 41/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 42/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 43/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 44/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 45/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 46/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 47/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 48/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 49/512\n",
      "183/183 [==============================] - 2s 12ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 50/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 51/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 52/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 53/512\n",
      "183/183 [==============================] - 2s 12ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 54/512\n",
      "183/183 [==============================] - 2s 12ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 55/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 56/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 57/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 58/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 59/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 60/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 61/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 62/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 63/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 64/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 65/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 66/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 67/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 68/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 69/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 70/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 71/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 72/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 73/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 74/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 75/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 76/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 77/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 78/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 79/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 80/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 81/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 82/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 83/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 84/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 85/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 86/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 87/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 88/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 89/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 90/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 91/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 92/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 93/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 94/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 95/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 96/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 97/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 98/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 99/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 100/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 101/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 102/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 103/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 104/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 105/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 106/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 107/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 108/512\n",
      "183/183 [==============================] - 2s 12ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 109/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 110/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 111/512\n",
      "183/183 [==============================] - 2s 12ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 112/512\n",
      "183/183 [==============================] - 2s 12ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 113/512\n",
      "183/183 [==============================] - 2s 12ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 114/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 115/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 116/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 117/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 118/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 119/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 120/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 121/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 122/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 123/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 124/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 125/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 126/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 127/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 128/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 129/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 130/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 131/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 132/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 133/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 134/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 135/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 136/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 137/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 138/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 139/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 140/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 141/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 142/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 143/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 144/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 145/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 146/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 147/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 148/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 149/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 150/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 151/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 152/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 153/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 154/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 155/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 156/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 157/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 158/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 159/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 160/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 161/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 162/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 163/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 164/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 165/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 166/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 167/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 168/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 169/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 170/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 171/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 172/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 173/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 174/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 175/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 176/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 177/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 178/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 179/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 180/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 181/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 182/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 183/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 184/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 185/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 186/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 187/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 188/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 189/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 190/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 191/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 192/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 193/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 194/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 195/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 196/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 197/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 198/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 199/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 200/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 201/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 202/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 203/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 204/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 205/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 206/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 207/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 208/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 209/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 210/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 211/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 212/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 213/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 214/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 215/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 216/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 217/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 218/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 219/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 220/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 221/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 222/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 223/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 224/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 225/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 226/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 227/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 228/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 229/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 230/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 231/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 232/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 233/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 234/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 235/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 236/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 237/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 238/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 239/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 240/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 241/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 242/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 243/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 244/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 245/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 246/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 247/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 248/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 249/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 250/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 251/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 252/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 253/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 254/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 255/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 256/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 257/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 258/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 259/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 260/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 261/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 262/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 263/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 264/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 265/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 266/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 267/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 268/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 269/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 270/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 271/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 272/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 273/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 274/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 275/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 276/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 277/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 278/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 279/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 280/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 281/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 282/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 283/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 284/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 285/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 286/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 287/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 288/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 289/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 290/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 291/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 292/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 293/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 294/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 295/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 296/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 297/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 298/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 299/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 300/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 301/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 302/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 303/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 304/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 305/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 306/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 307/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 308/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 309/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 310/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 311/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 312/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 313/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 314/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 315/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 316/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 317/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 318/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 319/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 320/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 321/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 322/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 323/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 324/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 325/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 326/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 327/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 328/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 329/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 330/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 331/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 332/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 333/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 334/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 335/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 336/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 337/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 338/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 339/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 340/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 341/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 342/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 343/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 344/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 345/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 346/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 347/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 348/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 349/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 350/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 351/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 352/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 353/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 354/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 355/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 356/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 357/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 358/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 359/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 360/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 361/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 362/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 363/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 364/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 365/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 366/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 367/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 368/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 369/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 370/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 371/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 372/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 373/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 374/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 375/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 376/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 377/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 378/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 379/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 380/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 381/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 382/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 383/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 384/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 385/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 386/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 387/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 388/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 389/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 390/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 391/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 392/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 393/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 394/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 395/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 396/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 397/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 398/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 399/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 400/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 401/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 402/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 403/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 404/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 405/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 406/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 407/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 408/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 409/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 410/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 411/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 412/512\n",
      "183/183 [==============================] - 2s 12ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 413/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 414/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 415/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 416/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 417/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 418/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 419/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 420/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 421/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 422/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 423/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 424/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 425/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 426/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 427/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 428/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 429/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 430/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 431/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 432/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 433/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 434/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 435/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 436/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 437/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 438/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 439/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 440/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 441/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 442/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 443/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 444/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 445/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 446/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 447/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 448/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 449/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 450/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 451/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 452/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 453/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 454/512\n",
      "183/183 [==============================] - 2s 12ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 455/512\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 456/512\n",
      "183/183 [==============================] - 2s 13ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 457/512\n",
      "183/183 [==============================] - 3s 17ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 458/512\n",
      "183/183 [==============================] - 3s 16ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 459/512\n",
      "183/183 [==============================] - 2s 13ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 460/512\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 461/512\n",
      "183/183 [==============================] - 3s 17ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 462/512\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 463/512\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 464/512\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 465/512\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.4406 - mse: 0.440 - 3s 14ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 466/512\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 467/512\n",
      "183/183 [==============================] - 2s 13ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 468/512\n",
      "183/183 [==============================] - 2s 13ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 469/512\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 470/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 471/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 472/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 473/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 474/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 475/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 476/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 477/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 478/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 479/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 480/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 481/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 482/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 483/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 484/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 485/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 486/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 487/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 488/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 489/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 490/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 491/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 492/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 493/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 494/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 495/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 496/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 497/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 498/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 499/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 500/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 501/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 502/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 503/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 504/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 505/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 506/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 507/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 508/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 509/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 510/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 511/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 512/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,epochs = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "1ac52ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32884207543233024"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally check\n",
    "y_pred=model.predict(X_valid)\n",
    "np.mean((y_pred-y_valid)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "10ac7077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3741744653920835"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=model.predict(X_test)\n",
    "np.mean((y_pred-y_test)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a70f2fb",
   "metadata": {},
   "source": [
    "# Copying code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c797cfb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5842, 5, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.reshape(X_train, (len(X_train), 5, 1))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "474c74f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 10)                480       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 491\n",
      "Trainable params: 491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "#model.add(layers.Dense(10, activation='relu',input_shape=[5,1]))\n",
    "model.add(layers.LSTM(10, activation='tanh',input_shape=[5,1]))\n",
    "model.add(layers.Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12083f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0382 - val_mse: 0.0382\n",
      "Epoch 2/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0454 - val_mse: 0.0454\n",
      "Epoch 3/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0373 - val_mse: 0.0373\n",
      "Epoch 4/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0374 - val_mse: 0.0374\n",
      "Epoch 5/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0383 - val_mse: 0.0383\n",
      "Epoch 6/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0399 - val_mse: 0.0399\n",
      "Epoch 7/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0384 - val_mse: 0.0384\n",
      "Epoch 8/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0383 - val_mse: 0.0383\n",
      "Epoch 9/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 10/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0398 - val_mse: 0.0398\n",
      "Epoch 11/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0402 - val_mse: 0.0402\n",
      "Epoch 12/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0385 - val_mse: 0.0385\n",
      "Epoch 13/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0394 - val_mse: 0.0394\n",
      "Epoch 14/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0365 - val_mse: 0.0365\n",
      "Epoch 15/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0421 - val_mse: 0.0421\n",
      "Epoch 16/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 17/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 18/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0488 - val_mse: 0.0488\n",
      "Epoch 19/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0474 - val_mse: 0.0474\n",
      "Epoch 20/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0450 - val_mse: 0.0450\n",
      "Epoch 21/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0395 - val_mse: 0.0395\n",
      "Epoch 22/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0396 - val_mse: 0.0396\n",
      "Epoch 23/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0389 - val_mse: 0.0389\n",
      "Epoch 24/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0475 - val_mse: 0.0475\n",
      "Epoch 25/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0404 - val_mse: 0.0404\n",
      "Epoch 26/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0422 - val_mse: 0.0422\n",
      "Epoch 27/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0379 - val_mse: 0.0379\n",
      "Epoch 28/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0377 - val_mse: 0.0377\n",
      "Epoch 29/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0515 - val_mse: 0.0515\n",
      "Epoch 30/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 31/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 32/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 33/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0389 - val_mse: 0.0389\n",
      "Epoch 34/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0399 - val_mse: 0.0399\n",
      "Epoch 35/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0373 - val_mse: 0.0373\n",
      "Epoch 36/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0376 - val_mse: 0.0376\n",
      "Epoch 37/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0436 - val_mse: 0.0436\n",
      "Epoch 38/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0394 - val_mse: 0.0394\n",
      "Epoch 39/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0385 - val_mse: 0.0385\n",
      "Epoch 40/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 41/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0370 - val_mse: 0.0370\n",
      "Epoch 42/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 43/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0406 - val_mse: 0.0406\n",
      "Epoch 44/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 45/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0381 - val_mse: 0.0381\n",
      "Epoch 46/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 47/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0341 - val_mse: 0.0341\n",
      "Epoch 48/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0372 - val_mse: 0.0372\n",
      "Epoch 49/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0395 - val_mse: 0.0395\n",
      "Epoch 50/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0378 - val_mse: 0.0378\n",
      "Epoch 51/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0385 - val_mse: 0.0385\n",
      "Epoch 52/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0447 - val_mse: 0.0447\n",
      "Epoch 53/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0377 - val_mse: 0.0377\n",
      "Epoch 54/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0410 - val_mse: 0.0410\n",
      "Epoch 55/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 56/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0364 - val_mse: 0.0364\n",
      "Epoch 57/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0376 - val_mse: 0.0376\n",
      "Epoch 58/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0478 - val_mse: 0.0478\n",
      "Epoch 59/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 60/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0366 - val_mse: 0.0366\n",
      "Epoch 61/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 62/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0441 - val_mse: 0.0441\n",
      "Epoch 63/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 64/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0408 - val_mse: 0.0408\n",
      "Epoch 65/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0558 - val_mse: 0.0558\n",
      "Epoch 66/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0400 - val_mse: 0.0400\n",
      "Epoch 67/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0412 - val_mse: 0.0412\n",
      "Epoch 68/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0418 - val_mse: 0.0418\n",
      "Epoch 69/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 70/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0370 - val_mse: 0.0370\n",
      "Epoch 71/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0437 - val_mse: 0.0437\n",
      "Epoch 72/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 73/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0374 - val_mse: 0.0374\n",
      "Epoch 74/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 75/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0389 - val_mse: 0.0389\n",
      "Epoch 76/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0409 - val_mse: 0.0409\n",
      "Epoch 77/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0317 - val_mse: 0.0317\n",
      "Epoch 78/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 79/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0385 - val_mse: 0.0385\n",
      "Epoch 80/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 81/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0394 - val_mse: 0.0394\n",
      "Epoch 82/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0379 - val_mse: 0.0379\n",
      "Epoch 83/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 84/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0403 - val_mse: 0.0403\n",
      "Epoch 85/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 86/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 87/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0343 - val_mse: 0.0343\n",
      "Epoch 88/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0261 - val_mse: 0.0261\n",
      "Epoch 89/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0369 - val_mse: 0.0369\n",
      "Epoch 90/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0449 - val_mse: 0.0449\n",
      "Epoch 91/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 92/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0259 - val_mse: 0.0259\n",
      "Epoch 93/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0300 - val_mse: 0.0300\n",
      "Epoch 94/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 95/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0293 - val_mse: 0.0293\n",
      "Epoch 96/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0295 - val_mse: 0.0295\n",
      "Epoch 97/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 98/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0432 - val_mse: 0.0432\n",
      "Epoch 99/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 100/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0373 - val_mse: 0.0373\n",
      "Epoch 101/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 102/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0310 - val_mse: 0.0310\n",
      "Epoch 103/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0385 - val_mse: 0.0385\n",
      "Epoch 104/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0316 - val_mse: 0.0316\n",
      "Epoch 105/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0303 - val_mse: 0.0303\n",
      "Epoch 106/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0313 - val_mse: 0.0313\n",
      "Epoch 107/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0289 - val_mse: 0.0289\n",
      "Epoch 108/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0371 - val_mse: 0.0371\n",
      "Epoch 109/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 110/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0478 - val_mse: 0.0478\n",
      "Epoch 111/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0300 - val_mse: 0.0300\n",
      "Epoch 112/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0285 - val_mse: 0.0285\n",
      "Epoch 113/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 114/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0305 - val_mse: 0.0305\n",
      "Epoch 115/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0263 - val_mse: 0.0263\n",
      "Epoch 116/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 117/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0275 - val_mse: 0.0275\n",
      "Epoch 118/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 119/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 120/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 121/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 122/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0395 - val_mse: 0.0395\n",
      "Epoch 123/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0300 - val_mse: 0.0300\n",
      "Epoch 124/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 125/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0406 - val_mse: 0.0406\n",
      "Epoch 126/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0343 - val_mse: 0.0343\n",
      "Epoch 127/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 128/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 129/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0388 - val_mse: 0.0388\n",
      "Epoch 130/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 131/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0441 - val_mse: 0.0441\n",
      "Epoch 132/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 133/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0301 - val_mse: 0.0301\n",
      "Epoch 134/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0306 - val_mse: 0.0306\n",
      "Epoch 135/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0317 - val_mse: 0.0317\n",
      "Epoch 136/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0283 - val_mse: 0.0283\n",
      "Epoch 137/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0413 - val_mse: 0.0413\n",
      "Epoch 138/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 139/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 140/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0363 - val_mse: 0.0363\n",
      "Epoch 141/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 142/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0262 - val_mse: 0.0262\n",
      "Epoch 143/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 144/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0235 - val_mse: 0.0235\n",
      "Epoch 145/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0512 - val_mse: 0.0512\n",
      "Epoch 146/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 147/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0261 - val_mse: 0.0261\n",
      "Epoch 148/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0430 - val_mse: 0.0430\n",
      "Epoch 149/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0365 - val_mse: 0.0365\n",
      "Epoch 150/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0239 - val_mse: 0.0239\n",
      "Epoch 151/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0276 - val_mse: 0.0276\n",
      "Epoch 152/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0243 - val_mse: 0.0243\n",
      "Epoch 153/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0269 - val_mse: 0.0269\n",
      "Epoch 154/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0296 - val_mse: 0.0296\n",
      "Epoch 155/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0371 - val_mse: 0.0371\n",
      "Epoch 156/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0248 - val_mse: 0.0248\n",
      "Epoch 157/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0300 - val_mse: 0.0300\n",
      "Epoch 158/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0283 - val_mse: 0.0283\n",
      "Epoch 159/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 160/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0365 - val_mse: 0.0365\n",
      "Epoch 161/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 162/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0219 - val_mse: 0.0219\n",
      "Epoch 163/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 164/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0246 - val_mse: 0.0246\n",
      "Epoch 165/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 166/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0484 - val_mse: 0.0484\n",
      "Epoch 167/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0363 - val_mse: 0.0363\n",
      "Epoch 168/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 169/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0374 - val_mse: 0.0374\n",
      "Epoch 170/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0425 - val_mse: 0.0425\n",
      "Epoch 171/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0342 - val_mse: 0.0342\n",
      "Epoch 172/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0310 - val_mse: 0.0310\n",
      "Epoch 173/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 174/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0300 - val_mse: 0.0300\n",
      "Epoch 175/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0232 - val_mse: 0.0232\n",
      "Epoch 176/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 177/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 178/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 179/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0275 - val_mse: 0.0275\n",
      "Epoch 180/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0233 - val_mse: 0.0233\n",
      "Epoch 181/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0302 - val_mse: 0.0302\n",
      "Epoch 182/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 183/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0374 - val_mse: 0.0374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 185/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0331 - val_mse: 0.0331\n",
      "Epoch 186/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 187/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0402 - val_mse: 0.0402\n",
      "Epoch 188/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 189/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0278 - val_mse: 0.0278\n",
      "Epoch 190/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0265 - val_mse: 0.0265\n",
      "Epoch 191/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0305 - val_mse: 0.0305\n",
      "Epoch 192/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 193/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0274 - val_mse: 0.0274\n",
      "Epoch 194/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0363 - val_mse: 0.0363\n",
      "Epoch 195/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0340 - val_mse: 0.0340\n",
      "Epoch 196/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 197/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0297 - val_mse: 0.0297\n",
      "Epoch 198/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0237 - val_mse: 0.0237\n",
      "Epoch 199/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 200/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0264 - val_mse: 0.0264\n",
      "Epoch 201/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0267 - val_mse: 0.0267\n",
      "Epoch 202/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0411 - val_mse: 0.0411\n",
      "Epoch 203/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0451 - val_mse: 0.0451\n",
      "Epoch 204/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 205/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0285 - val_mse: 0.0285\n",
      "Epoch 206/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0409 - val_mse: 0.0409\n",
      "Epoch 207/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0306 - val_mse: 0.0306\n",
      "Epoch 208/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0301 - val_mse: 0.0301\n",
      "Epoch 209/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 210/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 211/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0312 - val_mse: 0.0312\n",
      "Epoch 212/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0420 - val_mse: 0.0420\n",
      "Epoch 213/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0313 - val_mse: 0.0313\n",
      "Epoch 214/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0290 - val_mse: 0.0290\n",
      "Epoch 215/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0321 - val_mse: 0.0321\n",
      "Epoch 216/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0315 - val_mse: 0.0315\n",
      "Epoch 217/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0343 - val_mse: 0.0343\n",
      "Epoch 218/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 219/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0267 - val_mse: 0.0267\n",
      "Epoch 220/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0415 - val_mse: 0.0415\n",
      "Epoch 221/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0216 - val_mse: 0.0216\n",
      "Epoch 222/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0218 - val_mse: 0.0218\n",
      "Epoch 223/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0293 - val_mse: 0.0293\n",
      "Epoch 224/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 225/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0315 - val_mse: 0.0315\n",
      "Epoch 226/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0405 - val_mse: 0.0405\n",
      "Epoch 227/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0218 - val_mse: 0.0218\n",
      "Epoch 228/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0245 - val_mse: 0.0245\n",
      "Epoch 229/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0362 - val_mse: 0.0362\n",
      "Epoch 230/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0284 - val_mse: 0.0284\n",
      "Epoch 231/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0288 - val_mse: 0.0288\n",
      "Epoch 232/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0408 - val_mse: 0.0408\n",
      "Epoch 233/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0225 - val_mse: 0.0225\n",
      "Epoch 234/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0440 - val_mse: 0.0440\n",
      "Epoch 235/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0355 - val_mse: 0.0355\n",
      "Epoch 236/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 237/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0296 - val_mse: 0.0296\n",
      "Epoch 238/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 239/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 240/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 241/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0290 - val_mse: 0.0290\n",
      "Epoch 242/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0414 - val_mse: 0.0414\n",
      "Epoch 243/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0263 - val_mse: 0.0263\n",
      "Epoch 244/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0269 - val_mse: 0.0269\n",
      "Epoch 245/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 246/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0240 - val_mse: 0.0240\n",
      "Epoch 247/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0278 - val_mse: 0.0278\n",
      "Epoch 248/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0274 - val_mse: 0.0274\n",
      "Epoch 249/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0359 - val_mse: 0.0359\n",
      "Epoch 250/512\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0239 - val_mse: 0.0239\n",
      "Epoch 251/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0280 - val_mse: 0.0280\n",
      "Epoch 252/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0258 - val_mse: 0.0258\n",
      "Epoch 253/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 254/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 255/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 256/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 257/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0265 - val_mse: 0.0265\n",
      "Epoch 258/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0262 - val_mse: 0.0262\n",
      "Epoch 259/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0343 - val_mse: 0.03430s - loss: 0.0131 - mse: \n",
      "Epoch 260/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 261/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0265 - val_mse: 0.0265\n",
      "Epoch 262/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0236 - val_mse: 0.0236\n",
      "Epoch 263/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0320 - val_mse: 0.0320\n",
      "Epoch 264/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0306 - val_mse: 0.0306\n",
      "Epoch 265/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0291 - val_mse: 0.0291\n",
      "Epoch 266/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0259 - val_mse: 0.0259\n",
      "Epoch 267/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0311 - val_mse: 0.0311\n",
      "Epoch 268/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0260 - val_mse: 0.0260\n",
      "Epoch 269/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0248 - val_mse: 0.0248\n",
      "Epoch 270/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 271/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 272/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0275 - val_mse: 0.0275\n",
      "Epoch 273/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0291 - val_mse: 0.0291\n",
      "Epoch 274/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 275/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0343 - val_mse: 0.0343\n",
      "Epoch 276/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0218 - val_mse: 0.0218\n",
      "Epoch 277/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0451 - val_mse: 0.0451\n",
      "Epoch 278/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0309 - val_mse: 0.0309\n",
      "Epoch 279/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 280/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 281/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 282/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 283/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0363 - val_mse: 0.0363\n",
      "Epoch 284/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0309 - val_mse: 0.0309\n",
      "Epoch 285/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0270 - val_mse: 0.0270\n",
      "Epoch 286/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0241 - val_mse: 0.0241\n",
      "Epoch 287/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0295 - val_mse: 0.0295\n",
      "Epoch 288/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0277 - val_mse: 0.0277\n",
      "Epoch 289/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0417 - val_mse: 0.0417\n",
      "Epoch 290/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 291/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0340 - val_mse: 0.0340\n",
      "Epoch 292/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0316 - val_mse: 0.0316\n",
      "Epoch 293/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0263 - val_mse: 0.0263\n",
      "Epoch 294/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0307 - val_mse: 0.0307\n",
      "Epoch 295/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0264 - val_mse: 0.0264\n",
      "Epoch 296/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 297/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0234 - val_mse: 0.0234\n",
      "Epoch 298/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0289 - val_mse: 0.0289\n",
      "Epoch 299/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0262 - val_mse: 0.0262\n",
      "Epoch 300/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0310 - val_mse: 0.0310\n",
      "Epoch 301/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0309 - val_mse: 0.0309\n",
      "Epoch 302/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0278 - val_mse: 0.0278\n",
      "Epoch 303/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 304/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 305/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0278 - val_mse: 0.0278\n",
      "Epoch 306/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0253 - val_mse: 0.0253\n",
      "Epoch 307/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0276 - val_mse: 0.0276\n",
      "Epoch 308/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0315 - val_mse: 0.0315\n",
      "Epoch 309/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0287 - val_mse: 0.0287\n",
      "Epoch 310/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0254 - val_mse: 0.0254\n",
      "Epoch 311/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0253 - val_mse: 0.0253\n",
      "Epoch 312/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0257 - val_mse: 0.0257\n",
      "Epoch 313/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 314/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0324 - val_mse: 0.0324\n",
      "Epoch 315/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 316/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0276 - val_mse: 0.0276\n",
      "Epoch 317/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0311 - val_mse: 0.0311\n",
      "Epoch 318/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0284 - val_mse: 0.0284\n",
      "Epoch 319/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0272 - val_mse: 0.0272\n",
      "Epoch 320/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0245 - val_mse: 0.0245\n",
      "Epoch 321/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0315 - val_mse: 0.0315\n",
      "Epoch 322/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 323/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 324/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0284 - val_mse: 0.0284\n",
      "Epoch 325/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 326/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0272 - val_mse: 0.0272\n",
      "Epoch 327/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0276 - val_mse: 0.0276\n",
      "Epoch 328/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0256 - val_mse: 0.0256\n",
      "Epoch 329/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 330/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0451 - val_mse: 0.0451\n",
      "Epoch 331/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0245 - val_mse: 0.0245\n",
      "Epoch 332/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0254 - val_mse: 0.0254\n",
      "Epoch 333/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0367 - val_mse: 0.0367\n",
      "Epoch 334/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0287 - val_mse: 0.0287\n",
      "Epoch 335/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0402 - val_mse: 0.0402\n",
      "Epoch 336/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0340 - val_mse: 0.0340\n",
      "Epoch 337/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 338/512\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0283 - val_mse: 0.0283\n",
      "Epoch 339/512\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0317 - val_mse: 0.0317\n",
      "Epoch 340/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0373 - val_mse: 0.0373\n",
      "Epoch 341/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0223 - val_mse: 0.0223\n",
      "Epoch 342/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0277 - val_mse: 0.0277\n",
      "Epoch 343/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0334 - val_mse: 0.0334\n",
      "Epoch 344/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0317 - val_mse: 0.0317\n",
      "Epoch 345/512\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0124 - mse: 0.012 - 1s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0265 - val_mse: 0.0265\n",
      "Epoch 346/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0256 - val_mse: 0.0256\n",
      "Epoch 347/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 348/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0281 - val_mse: 0.0281\n",
      "Epoch 349/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0240 - val_mse: 0.0240\n",
      "Epoch 350/512\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0365 - val_mse: 0.0365\n",
      "Epoch 351/512\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0240 - val_mse: 0.0240\n",
      "Epoch 352/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 353/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0263 - val_mse: 0.0263\n",
      "Epoch 354/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 355/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0227 - val_mse: 0.0227\n",
      "Epoch 356/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0297 - val_mse: 0.0297\n",
      "Epoch 357/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0259 - val_mse: 0.0259\n",
      "Epoch 358/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0439 - val_mse: 0.0439\n",
      "Epoch 359/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 360/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 361/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0375 - val_mse: 0.0375\n",
      "Epoch 362/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0407 - val_mse: 0.0407\n",
      "Epoch 363/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0261 - val_mse: 0.0261\n",
      "Epoch 364/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0446 - val_mse: 0.0446\n",
      "Epoch 365/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0305 - val_mse: 0.0305\n",
      "Epoch 366/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0244 - val_mse: 0.0244\n",
      "Epoch 367/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0253 - val_mse: 0.0253\n",
      "Epoch 368/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0415 - val_mse: 0.0415\n",
      "Epoch 369/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 370/512\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0269 - val_mse: 0.0269\n",
      "Epoch 371/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0284 - val_mse: 0.0284\n",
      "Epoch 372/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0271 - val_mse: 0.0271\n",
      "Epoch 373/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0291 - val_mse: 0.0291\n",
      "Epoch 374/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0315 - val_mse: 0.0315\n",
      "Epoch 375/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0253 - val_mse: 0.0253\n",
      "Epoch 376/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0288 - val_mse: 0.0288\n",
      "Epoch 377/512\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0329 - val_mse: 0.0329\n",
      "Epoch 378/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0225 - val_mse: 0.0225\n",
      "Epoch 379/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0313 - val_mse: 0.0313\n",
      "Epoch 380/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 381/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0274 - val_mse: 0.0274\n",
      "Epoch 382/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0344 - val_mse: 0.0344\n",
      "Epoch 383/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 384/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 385/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0242 - val_mse: 0.0242\n",
      "Epoch 386/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0245 - val_mse: 0.0245\n",
      "Epoch 387/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0264 - val_mse: 0.0264\n",
      "Epoch 388/512\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0256 - val_mse: 0.0256\n",
      "Epoch 389/512\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0316 - val_mse: 0.0316\n",
      "Epoch 390/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0222 - val_mse: 0.0222\n",
      "Epoch 391/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0261 - val_mse: 0.0261\n",
      "Epoch 392/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0218 - val_mse: 0.0218\n",
      "Epoch 393/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0309 - val_mse: 0.0309\n",
      "Epoch 394/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0246 - val_mse: 0.0246\n",
      "Epoch 395/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0232 - val_mse: 0.0232\n",
      "Epoch 396/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0218 - val_mse: 0.0218\n",
      "Epoch 397/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 398/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 399/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0293 - val_mse: 0.0293\n",
      "Epoch 400/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 401/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0302 - val_mse: 0.0302\n",
      "Epoch 402/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 403/512\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0123 - mse: 0.0123- ETA: 0s - loss: 0.0125 - ms - 1s 5ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0297 - val_mse: 0.0297\n",
      "Epoch 404/512\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0374 - val_mse: 0.0374\n",
      "Epoch 405/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0317 - val_mse: 0.0317\n",
      "Epoch 406/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0235 - val_mse: 0.0235\n",
      "Epoch 407/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0231 - val_mse: 0.0231\n",
      "Epoch 408/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0296 - val_mse: 0.0296\n",
      "Epoch 409/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 410/512\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0265 - val_mse: 0.0265\n",
      "Epoch 411/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 412/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0233 - val_mse: 0.0233\n",
      "Epoch 413/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0237 - val_mse: 0.0237\n",
      "Epoch 414/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0220 - val_mse: 0.0220\n",
      "Epoch 415/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0269 - val_mse: 0.0269\n",
      "Epoch 416/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0274 - val_mse: 0.0274\n",
      "Epoch 417/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0239 - val_mse: 0.0239\n",
      "Epoch 418/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0198 - val_mse: 0.0198\n",
      "Epoch 419/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0332 - val_mse: 0.0332\n",
      "Epoch 420/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0271 - val_mse: 0.0271\n",
      "Epoch 421/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0365 - val_mse: 0.0365\n",
      "Epoch 422/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 423/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0235 - val_mse: 0.0235\n",
      "Epoch 424/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0267 - val_mse: 0.0267\n",
      "Epoch 425/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0302 - val_mse: 0.0302\n",
      "Epoch 426/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0311 - val_mse: 0.0311\n",
      "Epoch 427/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0278 - val_mse: 0.0278\n",
      "Epoch 428/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 429/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0709 - val_mse: 0.0709\n",
      "Epoch 430/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0256 - val_mse: 0.0256\n",
      "Epoch 431/512\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0266 - val_mse: 0.0266\n",
      "Epoch 432/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0222 - val_mse: 0.0222\n",
      "Epoch 433/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0424 - val_mse: 0.0424\n",
      "Epoch 434/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 435/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0279 - val_mse: 0.0279\n",
      "Epoch 436/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0246 - val_mse: 0.0246\n",
      "Epoch 437/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 438/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0265 - val_mse: 0.0265\n",
      "Epoch 439/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0367 - val_mse: 0.0367\n",
      "Epoch 440/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0207 - val_mse: 0.0207\n",
      "Epoch 441/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0217 - val_mse: 0.0217\n",
      "Epoch 442/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 443/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0285 - val_mse: 0.0285\n",
      "Epoch 444/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0257 - val_mse: 0.0257\n",
      "Epoch 445/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 446/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0273 - val_mse: 0.0273\n",
      "Epoch 447/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 448/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0318 - val_mse: 0.0318\n",
      "Epoch 449/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 450/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 451/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0317 - val_mse: 0.0317\n",
      "Epoch 452/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0233 - val_mse: 0.0233\n",
      "Epoch 453/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0344 - val_mse: 0.0344\n",
      "Epoch 454/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0306 - val_mse: 0.0306\n",
      "Epoch 455/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0303 - val_mse: 0.0303\n",
      "Epoch 456/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0261 - val_mse: 0.0261\n",
      "Epoch 457/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0289 - val_mse: 0.0289\n",
      "Epoch 458/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0237 - val_mse: 0.0237\n",
      "Epoch 459/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0272 - val_mse: 0.0272\n",
      "Epoch 460/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0258 - val_mse: 0.0258\n",
      "Epoch 461/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0460 - val_mse: 0.0460\n",
      "Epoch 462/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0423 - val_mse: 0.0423\n",
      "Epoch 463/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 464/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0296 - val_mse: 0.0296\n",
      "Epoch 465/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0256 - val_mse: 0.0256\n",
      "Epoch 466/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 467/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0245 - val_mse: 0.0245\n",
      "Epoch 468/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0205 - val_mse: 0.0205\n",
      "Epoch 469/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0216 - val_mse: 0.0216\n",
      "Epoch 470/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0249 - val_mse: 0.0249\n",
      "Epoch 471/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0293 - val_mse: 0.0293\n",
      "Epoch 472/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0290 - val_mse: 0.0290\n",
      "Epoch 473/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0283 - val_mse: 0.0283\n",
      "Epoch 474/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0272 - val_mse: 0.0272\n",
      "Epoch 475/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0224 - val_mse: 0.0224\n",
      "Epoch 476/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0259 - val_mse: 0.0259\n",
      "Epoch 477/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0307 - val_mse: 0.0307\n",
      "Epoch 478/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0246 - val_mse: 0.0246\n",
      "Epoch 479/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0238 - val_mse: 0.0238\n",
      "Epoch 480/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0468 - val_mse: 0.0468\n",
      "Epoch 481/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 482/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 483/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0287 - val_mse: 0.0287\n",
      "Epoch 484/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0233 - val_mse: 0.0233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 485/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0242 - val_mse: 0.0242\n",
      "Epoch 486/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 487/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0360 - val_mse: 0.0360\n",
      "Epoch 488/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0246 - val_mse: 0.0246\n",
      "Epoch 489/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0231 - val_mse: 0.0231\n",
      "Epoch 490/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0217 - val_mse: 0.0217\n",
      "Epoch 491/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0219 - val_mse: 0.0219\n",
      "Epoch 492/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0269 - val_mse: 0.0269\n",
      "Epoch 493/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0287 - val_mse: 0.0287\n",
      "Epoch 494/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0272 - val_mse: 0.0272\n",
      "Epoch 495/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0297 - val_mse: 0.0297\n",
      "Epoch 496/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0308 - val_mse: 0.0308\n",
      "Epoch 497/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0301 - val_mse: 0.0301\n",
      "Epoch 498/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0361 - val_mse: 0.0361\n",
      "Epoch 499/512\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 500/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0291 - val_mse: 0.0291\n",
      "Epoch 501/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0256 - val_mse: 0.0256\n",
      "Epoch 502/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 503/512\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0326 - val_mse: 0.0326\n",
      "Epoch 504/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0379 - val_mse: 0.0379\n",
      "Epoch 505/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0363 - val_mse: 0.0363\n",
      "Epoch 506/512\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0280 - val_mse: 0.0280\n",
      "Epoch 507/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0253 - val_mse: 0.0253\n",
      "Epoch 508/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0253 - val_mse: 0.0253\n",
      "Epoch 509/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0265 - val_mse: 0.0265\n",
      "Epoch 510/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0235 - val_mse: 0.0235\n",
      "Epoch 511/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0479 - val_mse: 0.0479\n",
      "Epoch 512/512\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0258 - val_mse: 0.0258\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',loss='mse', metrics=['mse'])\n",
    "history = model.fit(X_train,y_train,epochs = 512,validation_data=(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1abd0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8145737851434963"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally check\n",
    "#X_valid = np.reshape(X_valid, (len(X_valid), 5, 1))\n",
    "y_pred=model.predict(X_valid)\n",
    "np.mean((y_pred-y_train)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e12d7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.7746634], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2df05704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.879517"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "503e5096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_valid,color='red')\n",
    "plt.plot(y_pred,color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be98ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
