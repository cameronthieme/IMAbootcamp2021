{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c44f1b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1288267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb10994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF=pd.read_csv(\"../../Data/1999_2021/aggragate_daily_data_2000to2020.csv\",index_col=0)\n",
    "DF_rain=pd.read_csv(\"../../Data/historic_weather.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1469c8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>energy_charge</th>\n",
       "      <th>effluent_flow</th>\n",
       "      <th>influent_flow</th>\n",
       "      <th>volume_used</th>\n",
       "      <th>poured_flow</th>\n",
       "      <th>water_level</th>\n",
       "      <th>energy_generated</th>\n",
       "      <th>energy_stored</th>\n",
       "      <th>maximum_demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>457.1039</td>\n",
       "      <td>226.12</td>\n",
       "      <td>185.42</td>\n",
       "      <td>20.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.08</td>\n",
       "      <td>0.7217</td>\n",
       "      <td>20346.6165</td>\n",
       "      <td>25253.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>465.5526</td>\n",
       "      <td>255.12</td>\n",
       "      <td>459.28</td>\n",
       "      <td>21.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.06</td>\n",
       "      <td>0.8238</td>\n",
       "      <td>20944.1655</td>\n",
       "      <td>26388.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>553.4819</td>\n",
       "      <td>263.12</td>\n",
       "      <td>885.74</td>\n",
       "      <td>23.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.16</td>\n",
       "      <td>0.8334</td>\n",
       "      <td>21915.0000</td>\n",
       "      <td>29633.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>581.4778</td>\n",
       "      <td>373.12</td>\n",
       "      <td>1011.04</td>\n",
       "      <td>25.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.46</td>\n",
       "      <td>1.2159</td>\n",
       "      <td>23301.4890</td>\n",
       "      <td>30230.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>595.3905</td>\n",
       "      <td>483.12</td>\n",
       "      <td>916.95</td>\n",
       "      <td>27.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.76</td>\n",
       "      <td>1.5561</td>\n",
       "      <td>24564.5235</td>\n",
       "      <td>30661.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2000-01-06</td>\n",
       "      <td>608.0315</td>\n",
       "      <td>408.12</td>\n",
       "      <td>915.51</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.96</td>\n",
       "      <td>1.3913</td>\n",
       "      <td>25612.0605</td>\n",
       "      <td>31290.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000-01-07</td>\n",
       "      <td>609.8446</td>\n",
       "      <td>382.12</td>\n",
       "      <td>921.31</td>\n",
       "      <td>30.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>444.19</td>\n",
       "      <td>1.3220</td>\n",
       "      <td>26503.2705</td>\n",
       "      <td>30742.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000-01-08</td>\n",
       "      <td>548.7882</td>\n",
       "      <td>396.12</td>\n",
       "      <td>968.39</td>\n",
       "      <td>32.68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>444.43</td>\n",
       "      <td>1.3761</td>\n",
       "      <td>27489.4455</td>\n",
       "      <td>30555.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2000-01-09</td>\n",
       "      <td>500.3805</td>\n",
       "      <td>252.12</td>\n",
       "      <td>1238.09</td>\n",
       "      <td>36.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>444.68</td>\n",
       "      <td>0.9007</td>\n",
       "      <td>28263.0450</td>\n",
       "      <td>28498.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2000-01-10</td>\n",
       "      <td>610.5973</td>\n",
       "      <td>427.12</td>\n",
       "      <td>810.87</td>\n",
       "      <td>37.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>445.10</td>\n",
       "      <td>1.5339</td>\n",
       "      <td>28823.3385</td>\n",
       "      <td>31625.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2000-01-11</td>\n",
       "      <td>633.0189</td>\n",
       "      <td>452.12</td>\n",
       "      <td>742.89</td>\n",
       "      <td>38.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>445.26</td>\n",
       "      <td>1.6378</td>\n",
       "      <td>29096.5455</td>\n",
       "      <td>31716.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2000-01-12</td>\n",
       "      <td>638.5056</td>\n",
       "      <td>495.12</td>\n",
       "      <td>495.12</td>\n",
       "      <td>38.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>445.38</td>\n",
       "      <td>1.7674</td>\n",
       "      <td>29358.0645</td>\n",
       "      <td>31593.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2000-01-13</td>\n",
       "      <td>641.5576</td>\n",
       "      <td>528.12</td>\n",
       "      <td>601.21</td>\n",
       "      <td>38.51</td>\n",
       "      <td>10.0</td>\n",
       "      <td>445.38</td>\n",
       "      <td>1.8637</td>\n",
       "      <td>29546.5335</td>\n",
       "      <td>31833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2000-01-14</td>\n",
       "      <td>648.6482</td>\n",
       "      <td>347.12</td>\n",
       "      <td>641.08</td>\n",
       "      <td>39.49</td>\n",
       "      <td>23.0</td>\n",
       "      <td>445.41</td>\n",
       "      <td>1.1925</td>\n",
       "      <td>29629.8105</td>\n",
       "      <td>31384.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2000-01-15</td>\n",
       "      <td>589.5792</td>\n",
       "      <td>495.12</td>\n",
       "      <td>544.36</td>\n",
       "      <td>39.66</td>\n",
       "      <td>14.0</td>\n",
       "      <td>445.53</td>\n",
       "      <td>1.7370</td>\n",
       "      <td>29846.0385</td>\n",
       "      <td>30506.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  energy_charge  effluent_flow  influent_flow  volume_used  \\\n",
       "1   2000-01-01       457.1039         226.12         185.42        20.84   \n",
       "2   2000-01-02       465.5526         255.12         459.28        21.53   \n",
       "3   2000-01-03       553.4819         263.12         885.74        23.63   \n",
       "4   2000-01-04       581.4778         373.12        1011.04        25.78   \n",
       "5   2000-01-05       595.3905         483.12         916.95        27.24   \n",
       "6   2000-01-06       608.0315         408.12         915.51        28.95   \n",
       "7   2000-01-07       609.8446         382.12         921.31        30.76   \n",
       "8   2000-01-08       548.7882         396.12         968.39        32.68   \n",
       "9   2000-01-09       500.3805         252.12        1238.09        36.00   \n",
       "10  2000-01-10       610.5973         427.12         810.87        37.28   \n",
       "11  2000-01-11       633.0189         452.12         742.89        38.26   \n",
       "12  2000-01-12       638.5056         495.12         495.12        38.26   \n",
       "13  2000-01-13       641.5576         528.12         601.21        38.51   \n",
       "14  2000-01-14       648.6482         347.12         641.08        39.49   \n",
       "15  2000-01-15       589.5792         495.12         544.36        39.66   \n",
       "\n",
       "    poured_flow  water_level  energy_generated  energy_stored  maximum_demand  \n",
       "1           0.0       443.08            0.7217     20346.6165         25253.0  \n",
       "2           0.0       443.06            0.8238     20944.1655         26388.0  \n",
       "3           0.0       443.16            0.8334     21915.0000         29633.0  \n",
       "4           0.0       443.46            1.2159     23301.4890         30230.0  \n",
       "5           0.0       443.76            1.5561     24564.5235         30661.0  \n",
       "6           0.0       443.96            1.3913     25612.0605         31290.0  \n",
       "7           0.0       444.19            1.3220     26503.2705         30742.0  \n",
       "8           0.0       444.43            1.3761     27489.4455         30555.0  \n",
       "9           0.0       444.68            0.9007     28263.0450         28498.0  \n",
       "10          0.0       445.10            1.5339     28823.3385         31625.0  \n",
       "11          0.0       445.26            1.6378     29096.5455         31716.0  \n",
       "12          0.0       445.38            1.7674     29358.0645         31593.0  \n",
       "13         10.0       445.38            1.8637     29546.5335         31833.0  \n",
       "14         23.0       445.41            1.1925     29629.8105         31384.0  \n",
       "15         14.0       445.53            1.7370     29846.0385         30506.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972b0734",
   "metadata": {},
   "source": [
    "We will look at a simple RNN model to find a relation between water level and rain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "207eb376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>energy_charge</th>\n",
       "      <th>effluent_flow</th>\n",
       "      <th>influent_flow</th>\n",
       "      <th>volume_used</th>\n",
       "      <th>poured_flow</th>\n",
       "      <th>water_level</th>\n",
       "      <th>energy_generated</th>\n",
       "      <th>energy_stored</th>\n",
       "      <th>maximum_demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7303.000000</td>\n",
       "      <td>7303.000000</td>\n",
       "      <td>7303.000000</td>\n",
       "      <td>7303.000000</td>\n",
       "      <td>7303.000000</td>\n",
       "      <td>7303.000000</td>\n",
       "      <td>7303.000000</td>\n",
       "      <td>7303.000000</td>\n",
       "      <td>7303.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>761.050363</td>\n",
       "      <td>381.621946</td>\n",
       "      <td>382.973679</td>\n",
       "      <td>76.350307</td>\n",
       "      <td>41.320702</td>\n",
       "      <td>449.354220</td>\n",
       "      <td>1.494725</td>\n",
       "      <td>73455.053713</td>\n",
       "      <td>38328.306482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>126.663031</td>\n",
       "      <td>255.239604</td>\n",
       "      <td>313.443189</td>\n",
       "      <td>15.452552</td>\n",
       "      <td>152.686579</td>\n",
       "      <td>1.479972</td>\n",
       "      <td>0.649623</td>\n",
       "      <td>27714.395965</td>\n",
       "      <td>5295.560131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>381.081260</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.900000</td>\n",
       "      <td>20.840000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>443.060000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20346.616500</td>\n",
       "      <td>20938.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>667.442940</td>\n",
       "      <td>241.000000</td>\n",
       "      <td>191.920000</td>\n",
       "      <td>62.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>448.100000</td>\n",
       "      <td>1.014822</td>\n",
       "      <td>51303.060655</td>\n",
       "      <td>34636.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>764.835851</td>\n",
       "      <td>322.000000</td>\n",
       "      <td>277.310000</td>\n",
       "      <td>78.930000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>449.660000</td>\n",
       "      <td>1.394540</td>\n",
       "      <td>70769.379000</td>\n",
       "      <td>38890.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>855.557605</td>\n",
       "      <td>435.000000</td>\n",
       "      <td>465.990000</td>\n",
       "      <td>89.740000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>450.630000</td>\n",
       "      <td>1.879905</td>\n",
       "      <td>97575.807000</td>\n",
       "      <td>42158.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1120.809714</td>\n",
       "      <td>2844.000000</td>\n",
       "      <td>3606.540000</td>\n",
       "      <td>100.360000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>451.530000</td>\n",
       "      <td>3.251000</td>\n",
       "      <td>131011.522500</td>\n",
       "      <td>54043.180000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       energy_charge  effluent_flow  influent_flow  volume_used  poured_flow  \\\n",
       "count    7303.000000    7303.000000    7303.000000  7303.000000  7303.000000   \n",
       "mean      761.050363     381.621946     382.973679    76.350307    41.320702   \n",
       "std       126.663031     255.239604     313.443189    15.452552   152.686579   \n",
       "min       381.081260      17.000000      18.900000    20.840000     0.000000   \n",
       "25%       667.442940     241.000000     191.920000    62.800000     0.000000   \n",
       "50%       764.835851     322.000000     277.310000    78.930000     0.000000   \n",
       "75%       855.557605     435.000000     465.990000    89.740000    25.000000   \n",
       "max      1120.809714    2844.000000    3606.540000   100.360000  2100.000000   \n",
       "\n",
       "       water_level  energy_generated  energy_stored  maximum_demand  \n",
       "count  7303.000000       7303.000000    7303.000000     7303.000000  \n",
       "mean    449.354220          1.494725   73455.053713    38328.306482  \n",
       "std       1.479972          0.649623   27714.395965     5295.560131  \n",
       "min     443.060000          0.000000   20346.616500    20938.000000  \n",
       "25%     448.100000          1.014822   51303.060655    34636.270000  \n",
       "50%     449.660000          1.394540   70769.379000    38890.140000  \n",
       "75%     450.630000          1.879905   97575.807000    42158.950000  \n",
       "max     451.530000          3.251000  131011.522500    54043.180000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "034ea7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date        datetime64[ns]\n",
       "hour               float64\n",
       "station             object\n",
       "rain_mm            float64\n",
       "temp_max           float64\n",
       "temp_min           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.date=pd.to_datetime(DF.date)\n",
    "DF_rain.date=pd.to_datetime(DF_rain.date)\n",
    "DF_rain.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc1d9250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create some lags\n",
    "def sum2(x):\n",
    "    if all(x.isna()):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return np.sum(x)\n",
    "\n",
    "\n",
    "KK=DF_rain.groupby(['date','station']).agg(rain_mm=('rain_mm',sum2)).reset_index().sort_values(['station','date'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b53ed543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A705 BAURU</th>\n",
       "      <th>A711 SAO CARLOS</th>\n",
       "      <th>A737 IBITINGA</th>\n",
       "      <th>A741 BARRA BONITA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-01-01</th>\n",
       "      <td>40.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-02</th>\n",
       "      <td>48.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-03</th>\n",
       "      <td>18.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-04</th>\n",
       "      <td>1.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-05</th>\n",
       "      <td>29.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-27</th>\n",
       "      <td>51.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-28</th>\n",
       "      <td>29.8</td>\n",
       "      <td>29.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-29</th>\n",
       "      <td>62.4</td>\n",
       "      <td>12.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-30</th>\n",
       "      <td>15.0</td>\n",
       "      <td>24.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31</th>\n",
       "      <td>10.8</td>\n",
       "      <td>27.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5114 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            A705 BAURU  A711 SAO CARLOS  A737 IBITINGA  A741 BARRA BONITA\n",
       "date                                                                     \n",
       "2007-01-01        40.6              NaN            NaN                NaN\n",
       "2007-01-02        48.8              NaN            NaN                NaN\n",
       "2007-01-03        18.2              NaN            NaN                NaN\n",
       "2007-01-04         1.4              NaN            NaN                NaN\n",
       "2007-01-05        29.4              NaN            NaN                NaN\n",
       "...                ...              ...            ...                ...\n",
       "2020-12-27        51.0             12.8            0.0                0.0\n",
       "2020-12-28        29.8             29.4            0.0                0.0\n",
       "2020-12-29        62.4             12.6            0.0                0.0\n",
       "2020-12-30        15.0             24.4            0.0                0.0\n",
       "2020-12-31        10.8             27.4            0.0                0.0\n",
       "\n",
       "[5114 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rain_pivot=KK.pivot(index='date', columns='station', values='rain_mm')\n",
    "Rain_pivot.columns.name=None\n",
    "Rain_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "399c6856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>energy_charge</th>\n",
       "      <th>effluent_flow</th>\n",
       "      <th>influent_flow</th>\n",
       "      <th>volume_used</th>\n",
       "      <th>poured_flow</th>\n",
       "      <th>water_level</th>\n",
       "      <th>energy_generated</th>\n",
       "      <th>energy_stored</th>\n",
       "      <th>maximum_demand</th>\n",
       "      <th>A705 BAURU</th>\n",
       "      <th>A711 SAO CARLOS</th>\n",
       "      <th>A737 IBITINGA</th>\n",
       "      <th>A741 BARRA BONITA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-01</th>\n",
       "      <td>457.103900</td>\n",
       "      <td>226.12</td>\n",
       "      <td>185.42</td>\n",
       "      <td>20.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.08</td>\n",
       "      <td>0.721700</td>\n",
       "      <td>20346.61650</td>\n",
       "      <td>25253.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-02</th>\n",
       "      <td>465.552600</td>\n",
       "      <td>255.12</td>\n",
       "      <td>459.28</td>\n",
       "      <td>21.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.06</td>\n",
       "      <td>0.823800</td>\n",
       "      <td>20944.16550</td>\n",
       "      <td>26388.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>553.481900</td>\n",
       "      <td>263.12</td>\n",
       "      <td>885.74</td>\n",
       "      <td>23.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.16</td>\n",
       "      <td>0.833400</td>\n",
       "      <td>21915.00000</td>\n",
       "      <td>29633.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>581.477800</td>\n",
       "      <td>373.12</td>\n",
       "      <td>1011.04</td>\n",
       "      <td>25.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.46</td>\n",
       "      <td>1.215900</td>\n",
       "      <td>23301.48900</td>\n",
       "      <td>30230.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>595.390500</td>\n",
       "      <td>483.12</td>\n",
       "      <td>916.95</td>\n",
       "      <td>27.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.76</td>\n",
       "      <td>1.556100</td>\n",
       "      <td>24564.52350</td>\n",
       "      <td>30661.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>912.189387</td>\n",
       "      <td>323.00</td>\n",
       "      <td>323.00</td>\n",
       "      <td>53.89</td>\n",
       "      <td>25.0</td>\n",
       "      <td>447.17</td>\n",
       "      <td>1.163433</td>\n",
       "      <td>30146.37673</td>\n",
       "      <td>43231.117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-28</th>\n",
       "      <td>880.473677</td>\n",
       "      <td>375.00</td>\n",
       "      <td>209.72</td>\n",
       "      <td>53.34</td>\n",
       "      <td>25.0</td>\n",
       "      <td>447.11</td>\n",
       "      <td>1.362688</td>\n",
       "      <td>30230.79799</td>\n",
       "      <td>43006.734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-29</th>\n",
       "      <td>827.558858</td>\n",
       "      <td>299.00</td>\n",
       "      <td>216.71</td>\n",
       "      <td>53.06</td>\n",
       "      <td>25.0</td>\n",
       "      <td>447.08</td>\n",
       "      <td>1.058915</td>\n",
       "      <td>30351.00804</td>\n",
       "      <td>40620.258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>893.432298</td>\n",
       "      <td>345.00</td>\n",
       "      <td>208.08</td>\n",
       "      <td>52.60</td>\n",
       "      <td>25.0</td>\n",
       "      <td>447.03</td>\n",
       "      <td>1.256731</td>\n",
       "      <td>30155.61356</td>\n",
       "      <td>41780.441</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>850.271015</td>\n",
       "      <td>148.00</td>\n",
       "      <td>175.31</td>\n",
       "      <td>52.69</td>\n",
       "      <td>25.0</td>\n",
       "      <td>447.04</td>\n",
       "      <td>0.505311</td>\n",
       "      <td>30023.23611</td>\n",
       "      <td>42182.234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7303 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            energy_charge  effluent_flow  influent_flow  volume_used  \\\n",
       "date                                                                   \n",
       "2000-01-01     457.103900         226.12         185.42        20.84   \n",
       "2000-01-02     465.552600         255.12         459.28        21.53   \n",
       "2000-01-03     553.481900         263.12         885.74        23.63   \n",
       "2000-01-04     581.477800         373.12        1011.04        25.78   \n",
       "2000-01-05     595.390500         483.12         916.95        27.24   \n",
       "...                   ...            ...            ...          ...   \n",
       "2019-12-27     912.189387         323.00         323.00        53.89   \n",
       "2019-12-28     880.473677         375.00         209.72        53.34   \n",
       "2019-12-29     827.558858         299.00         216.71        53.06   \n",
       "2019-12-30     893.432298         345.00         208.08        52.60   \n",
       "2019-12-31     850.271015         148.00         175.31        52.69   \n",
       "\n",
       "            poured_flow  water_level  energy_generated  energy_stored  \\\n",
       "date                                                                    \n",
       "2000-01-01          0.0       443.08          0.721700    20346.61650   \n",
       "2000-01-02          0.0       443.06          0.823800    20944.16550   \n",
       "2000-01-03          0.0       443.16          0.833400    21915.00000   \n",
       "2000-01-04          0.0       443.46          1.215900    23301.48900   \n",
       "2000-01-05          0.0       443.76          1.556100    24564.52350   \n",
       "...                 ...          ...               ...            ...   \n",
       "2019-12-27         25.0       447.17          1.163433    30146.37673   \n",
       "2019-12-28         25.0       447.11          1.362688    30230.79799   \n",
       "2019-12-29         25.0       447.08          1.058915    30351.00804   \n",
       "2019-12-30         25.0       447.03          1.256731    30155.61356   \n",
       "2019-12-31         25.0       447.04          0.505311    30023.23611   \n",
       "\n",
       "            maximum_demand  A705 BAURU  A711 SAO CARLOS  A737 IBITINGA  \\\n",
       "date                                                                     \n",
       "2000-01-01       25253.000         NaN              NaN            NaN   \n",
       "2000-01-02       26388.000         NaN              NaN            NaN   \n",
       "2000-01-03       29633.000         NaN              NaN            NaN   \n",
       "2000-01-04       30230.000         NaN              NaN            NaN   \n",
       "2000-01-05       30661.000         NaN              NaN            NaN   \n",
       "...                    ...         ...              ...            ...   \n",
       "2019-12-27       43231.117         0.0              0.0            NaN   \n",
       "2019-12-28       43006.734         0.0              9.0            NaN   \n",
       "2019-12-29       40620.258         0.0              0.0            NaN   \n",
       "2019-12-30       41780.441         0.0              0.0            NaN   \n",
       "2019-12-31       42182.234         0.0              0.8            NaN   \n",
       "\n",
       "            A741 BARRA BONITA  \n",
       "date                           \n",
       "2000-01-01                NaN  \n",
       "2000-01-02                NaN  \n",
       "2000-01-03                NaN  \n",
       "2000-01-04                NaN  \n",
       "2000-01-05                NaN  \n",
       "...                       ...  \n",
       "2019-12-27                NaN  \n",
       "2019-12-28                NaN  \n",
       "2019-12-29                NaN  \n",
       "2019-12-30                NaN  \n",
       "2019-12-31                NaN  \n",
       "\n",
       "[7303 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combine Both\n",
    "DF=DF.merge(Rain_pivot,how='left', on='date')\n",
    "DF=DF.set_index('date')\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77f31f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_shift=0\n",
    "shifts=range(1,max_shift+1)\n",
    "Shifted=[]\n",
    "for shift in shifts:\n",
    "    DF_shifted=DF.shift(periods=shift)\n",
    "    cols=DF_shifted.columns\n",
    "    DF_shifted.columns=[name+'_'+str(shift) for name in cols]\n",
    "    Shifted.append(DF_shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ea84f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for DF_shifted in Shifted:\n",
    "    DF=DF.merge(DF_shifted, on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd0d2c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF=DF.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6e0e509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>energy_charge</th>\n",
       "      <th>effluent_flow</th>\n",
       "      <th>influent_flow</th>\n",
       "      <th>volume_used</th>\n",
       "      <th>poured_flow</th>\n",
       "      <th>water_level</th>\n",
       "      <th>energy_generated</th>\n",
       "      <th>energy_stored</th>\n",
       "      <th>maximum_demand</th>\n",
       "      <th>A705 BAURU</th>\n",
       "      <th>A711 SAO CARLOS</th>\n",
       "      <th>A737 IBITINGA</th>\n",
       "      <th>A741 BARRA BONITA</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>month_day</th>\n",
       "      <th>year_day</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>457.103900</td>\n",
       "      <td>226.12</td>\n",
       "      <td>185.42</td>\n",
       "      <td>20.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.08</td>\n",
       "      <td>0.721700</td>\n",
       "      <td>20346.61650</td>\n",
       "      <td>25253.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>465.552600</td>\n",
       "      <td>255.12</td>\n",
       "      <td>459.28</td>\n",
       "      <td>21.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.06</td>\n",
       "      <td>0.823800</td>\n",
       "      <td>20944.16550</td>\n",
       "      <td>26388.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>553.481900</td>\n",
       "      <td>263.12</td>\n",
       "      <td>885.74</td>\n",
       "      <td>23.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.16</td>\n",
       "      <td>0.833400</td>\n",
       "      <td>21915.00000</td>\n",
       "      <td>29633.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>581.477800</td>\n",
       "      <td>373.12</td>\n",
       "      <td>1011.04</td>\n",
       "      <td>25.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.46</td>\n",
       "      <td>1.215900</td>\n",
       "      <td>23301.48900</td>\n",
       "      <td>30230.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>595.390500</td>\n",
       "      <td>483.12</td>\n",
       "      <td>916.95</td>\n",
       "      <td>27.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.76</td>\n",
       "      <td>1.556100</td>\n",
       "      <td>24564.52350</td>\n",
       "      <td>30661.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7298</th>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>912.189387</td>\n",
       "      <td>323.00</td>\n",
       "      <td>323.00</td>\n",
       "      <td>53.89</td>\n",
       "      <td>25.0</td>\n",
       "      <td>447.17</td>\n",
       "      <td>1.163433</td>\n",
       "      <td>30146.37673</td>\n",
       "      <td>43231.117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>361</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7299</th>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>880.473677</td>\n",
       "      <td>375.00</td>\n",
       "      <td>209.72</td>\n",
       "      <td>53.34</td>\n",
       "      <td>25.0</td>\n",
       "      <td>447.11</td>\n",
       "      <td>1.362688</td>\n",
       "      <td>30230.79799</td>\n",
       "      <td>43006.734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>362</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7300</th>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>827.558858</td>\n",
       "      <td>299.00</td>\n",
       "      <td>216.71</td>\n",
       "      <td>53.06</td>\n",
       "      <td>25.0</td>\n",
       "      <td>447.08</td>\n",
       "      <td>1.058915</td>\n",
       "      <td>30351.00804</td>\n",
       "      <td>40620.258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>363</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7301</th>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>893.432298</td>\n",
       "      <td>345.00</td>\n",
       "      <td>208.08</td>\n",
       "      <td>52.60</td>\n",
       "      <td>25.0</td>\n",
       "      <td>447.03</td>\n",
       "      <td>1.256731</td>\n",
       "      <td>30155.61356</td>\n",
       "      <td>41780.441</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7302</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>850.271015</td>\n",
       "      <td>148.00</td>\n",
       "      <td>175.31</td>\n",
       "      <td>52.69</td>\n",
       "      <td>25.0</td>\n",
       "      <td>447.04</td>\n",
       "      <td>0.505311</td>\n",
       "      <td>30023.23611</td>\n",
       "      <td>42182.234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>365</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7303 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  energy_charge  effluent_flow  influent_flow  volume_used  \\\n",
       "0    2000-01-01     457.103900         226.12         185.42        20.84   \n",
       "1    2000-01-02     465.552600         255.12         459.28        21.53   \n",
       "2    2000-01-03     553.481900         263.12         885.74        23.63   \n",
       "3    2000-01-04     581.477800         373.12        1011.04        25.78   \n",
       "4    2000-01-05     595.390500         483.12         916.95        27.24   \n",
       "...         ...            ...            ...            ...          ...   \n",
       "7298 2019-12-27     912.189387         323.00         323.00        53.89   \n",
       "7299 2019-12-28     880.473677         375.00         209.72        53.34   \n",
       "7300 2019-12-29     827.558858         299.00         216.71        53.06   \n",
       "7301 2019-12-30     893.432298         345.00         208.08        52.60   \n",
       "7302 2019-12-31     850.271015         148.00         175.31        52.69   \n",
       "\n",
       "      poured_flow  water_level  energy_generated  energy_stored  \\\n",
       "0             0.0       443.08          0.721700    20346.61650   \n",
       "1             0.0       443.06          0.823800    20944.16550   \n",
       "2             0.0       443.16          0.833400    21915.00000   \n",
       "3             0.0       443.46          1.215900    23301.48900   \n",
       "4             0.0       443.76          1.556100    24564.52350   \n",
       "...           ...          ...               ...            ...   \n",
       "7298         25.0       447.17          1.163433    30146.37673   \n",
       "7299         25.0       447.11          1.362688    30230.79799   \n",
       "7300         25.0       447.08          1.058915    30351.00804   \n",
       "7301         25.0       447.03          1.256731    30155.61356   \n",
       "7302         25.0       447.04          0.505311    30023.23611   \n",
       "\n",
       "      maximum_demand  A705 BAURU  A711 SAO CARLOS  A737 IBITINGA  \\\n",
       "0          25253.000         NaN              NaN            NaN   \n",
       "1          26388.000         NaN              NaN            NaN   \n",
       "2          29633.000         NaN              NaN            NaN   \n",
       "3          30230.000         NaN              NaN            NaN   \n",
       "4          30661.000         NaN              NaN            NaN   \n",
       "...              ...         ...              ...            ...   \n",
       "7298       43231.117         0.0              0.0            NaN   \n",
       "7299       43006.734         0.0              9.0            NaN   \n",
       "7300       40620.258         0.0              0.0            NaN   \n",
       "7301       41780.441         0.0              0.0            NaN   \n",
       "7302       42182.234         0.0              0.8            NaN   \n",
       "\n",
       "      A741 BARRA BONITA  year  month  month_day  year_day  weekday  \n",
       "0                   NaN  2000      1          1         1        5  \n",
       "1                   NaN  2000      1          2         2        6  \n",
       "2                   NaN  2000      1          3         3        0  \n",
       "3                   NaN  2000      1          4         4        1  \n",
       "4                   NaN  2000      1          5         5        2  \n",
       "...                 ...   ...    ...        ...       ...      ...  \n",
       "7298                NaN  2019     12         27       361        4  \n",
       "7299                NaN  2019     12         28       362        5  \n",
       "7300                NaN  2019     12         29       363        6  \n",
       "7301                NaN  2019     12         30       364        0  \n",
       "7302                NaN  2019     12         31       365        1  \n",
       "\n",
       "[7303 rows x 19 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF['year']=0\n",
    "DF['month']=0\n",
    "DF['month_day']=0\n",
    "DF['year_day']=0\n",
    "DF['weekday']=0\n",
    "\n",
    "for ind in DF.index:\n",
    "    date=DF.date[ind]\n",
    "    date_st=date.timetuple()\n",
    "    DF.loc[ind,'year']=date_st.tm_year\n",
    "    DF.loc[ind,'month']=date_st.tm_mon\n",
    "    DF.loc[ind,'month_day']=date_st.tm_mday\n",
    "    DF.loc[ind,'year_day']=date_st.tm_yday\n",
    "    DF.loc[ind,'weekday']=date_st.tm_wday\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c3ad964",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF=DF.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af891e83",
   "metadata": {},
   "source": [
    "# Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9a041ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_train='2016-01-01'\n",
    "date_validate='2018-01-01'\n",
    "\n",
    "DF=DF[['energy_generated','water_level','month','effluent_flow','maximum_demand','poured_flow','date']]\n",
    "\n",
    "y_var='energy_generated'\n",
    "X_train=np.array(DF.drop(['date',y_var],axis=1)[DF.date<date_train])\n",
    "y_train=np.array(DF[y_var][DF.date<date_train])\n",
    "\n",
    "X_valid=np.array(DF.drop(['date',y_var],axis=1)[(date_train<=DF.date) & (DF.date<date_validate)])\n",
    "y_valid=np.array(DF[y_var][(date_train<=DF.date) & (DF.date<date_validate)])\n",
    "\n",
    "X_test=np.array(DF.drop(['date',y_var],axis=1)[DF.date>=date_validate])\n",
    "y_test=np.array(DF[y_var][DF.date>=date_validate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c4997b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32814263259348403"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Forecast the mean\n",
    "y_pred=np.mean(y_train)\n",
    "np.mean((y_pred-y_valid)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b4b33c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17149029714665912"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Naive forecast using the previous day\n",
    "y_pred=np.array([y_train[-1]]+list(y_valid[:-1]))\n",
    "np.mean((y_pred-y_valid)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "955fc68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6316826677334865"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Naive forecast error using one month ago\n",
    "y_pred=np.array(list(y_train[-30:])+list(y_valid[:-30]))\n",
    "np.mean((y_pred-y_valid)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5025ed",
   "metadata": {},
   "source": [
    "## Regular NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0355f729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b3abb415",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(1000, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(1000, activation='tanh'))\n",
    "model.add(layers.Dense(1000, activation='relu'))\n",
    "model.add(layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c3626f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 1000)              6000      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 1001      \n",
      "=================================================================\n",
      "Total params: 2,009,001\n",
      "Trainable params: 2,009,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4b904fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we compile the network like so\n",
    "model.compile(optimizer='rmsprop',loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d0084ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected DataType for argument 'Tout' not CategoricalDtype(categories=[(0.0265, 0.674], (0.674, 1.318], (1.318, 1.962], (1.962, 2.607], (2.607, 3.251]],\n, ordered=True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mmake_type\u001b[1;34m(v, arg_name)\u001b[0m\n\u001b[0;32m    192\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m     \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py\u001b[0m in \u001b[0;36mas_dtype\u001b[1;34m(type_value)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m   raise TypeError(\"Cannot convert value %r to a TensorFlow DType.\" %\n\u001b[0m\u001b[0;32m    650\u001b[0m                   (type_value,))\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot convert value CategoricalDtype(categories=[(0.0265, 0.674], (0.674, 1.318], (1.318, 1.962], (1.962, 2.607], (2.607, 3.251]],\n, ordered=True) to a TensorFlow DType.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-125deaaee8a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m history = model.fit(X_train,y_train,epochs = 100,\n\u001b[0m\u001b[0;32m      2\u001b[0m                         validation_data=(X_valid,y_valid))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1047\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m       \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[0;32m   1050\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m           \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1105\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1106\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    474\u001b[0m         \"recommend using that to load a Dataset instead.\")\n\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGenericArrayLikeDataAdapter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"batch\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    518\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m     dataset = indices_dataset.map(\n\u001b[0m\u001b[0;32m    521\u001b[0m         grab_batch, num_parallel_calls=dataset_ops.AUTOTUNE)\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[0;32m   1695\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1696\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1697\u001b[1;33m       return ParallelMapDataset(\n\u001b[0m\u001b[0;32m   1698\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1699\u001b[0m           \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   4078\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4079\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4080\u001b[1;33m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[0;32m   4081\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4082\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3369\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3370\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3371\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3373\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2936\u001b[0m       \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mto\u001b[0m \u001b[0mspecialize\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2937\u001b[0m     \"\"\"\n\u001b[1;32m-> 2938\u001b[1;33m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[0;32m   2939\u001b[0m         *args, **kwargs)\n\u001b[0;32m   2940\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2904\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2906\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2907\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2908\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3362\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m   3363\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3364\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3365\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3366\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3297\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3299\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3300\u001b[0m       \u001b[1;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3301\u001b[0m       \u001b[1;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_whitelisted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m   \u001b[1;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mgrab_batch\u001b[1;34m(indices)\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mslice_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0minp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mflat_inputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m       \u001b[0mflat_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscript_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager_py_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_dtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_inp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdynamic_shape_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_inp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\u001b[0m in \u001b[0;36meager_py_func\u001b[1;34m(func, inp, Tout, name)\u001b[0m\n\u001b[0;32m    454\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost_address_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 456\u001b[1;33m       return _internal_py_func(\n\u001b[0m\u001b[0;32m    457\u001b[0m           func=func, inp=inp, Tout=Tout, eager=True, name=name)\n\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\u001b[0m in \u001b[0;36m_internal_py_func\u001b[1;34m(func, inp, Tout, stateful, eager, is_grad_func, name)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0meager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m     result = gen_script_ops.eager_py_func(\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[0mtoken\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\u001b[0m in \u001b[0;36meager_py_func\u001b[1;34m(input, token, Tout, is_async, name)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;34m\"Expected list for 'Tout' argument to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \"'eager_py_func' Op, not %r.\" % Tout)\n\u001b[1;32m---> 63\u001b[1;33m   \u001b[0mTout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Tout\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_t\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mTout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mis_async\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mis_async\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;34m\"Expected list for 'Tout' argument to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \"'eager_py_func' Op, not %r.\" % Tout)\n\u001b[1;32m---> 63\u001b[1;33m   \u001b[0mTout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Tout\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_t\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mTout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mis_async\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mis_async\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mmake_type\u001b[1;34m(v, arg_name)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m     raise TypeError(\"Expected DataType for argument '%s' not %s.\" %\n\u001b[0m\u001b[0;32m    196\u001b[0m                     (arg_name, repr(v)))\n\u001b[0;32m    197\u001b[0m   \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected DataType for argument 'Tout' not CategoricalDtype(categories=[(0.0265, 0.674], (0.674, 1.318], (1.318, 1.962], (1.962, 2.607], (2.607, 3.251]],\n, ordered=True)."
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,epochs = 100,\n",
    "                        validation_data=(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ae711e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3448478751997961"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally check\n",
    "y_pred=model.predict(X_valid)\n",
    "np.mean((y_pred-y_valid)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b5d5d6c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3628474442412001"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=model.predict(X_test)\n",
    "np.mean((y_pred-y_test)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca349daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "0edc1db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3ff061fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_39 (Dense)             (None, 512)               53760     \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 646,817\n",
      "Trainable params: 646,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "fff4371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we compile the network like so\n",
    "model.compile(optimizer='rmsprop',loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "9be6a391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 10938714.0000 - mse: 10938714.0000\n",
      "Epoch 2/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 29488.8027 - mse: 29488.8027\n",
      "Epoch 3/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 3055.8569 - mse: 3055.8569\n",
      "Epoch 4/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 4841.5732 - mse: 4841.5732\n",
      "Epoch 5/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 154.9090 - mse: 154.9090\n",
      "Epoch 6/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 27.8604 - mse: 27.8604\n",
      "Epoch 7/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 2.0118 - mse: 2.0118\n",
      "Epoch 8/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 1.5187 - mse: 1.5187\n",
      "Epoch 9/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 1.1788 - mse: 1.1788\n",
      "Epoch 10/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.9043 - mse: 0.9043\n",
      "Epoch 11/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.6950 - mse: 0.6950\n",
      "Epoch 12/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.5512 - mse: 0.5512\n",
      "Epoch 13/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4696 - mse: 0.4696\n",
      "Epoch 14/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4432 - mse: 0.4432\n",
      "Epoch 15/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4400 - mse: 0.4400\n",
      "Epoch 16/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 17/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 18/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 19/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 20/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 21/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 22/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 23/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 24/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 25/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 26/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 27/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 28/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 29/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 30/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 31/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 32/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 33/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 34/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 35/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 36/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 37/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 38/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 39/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 40/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 41/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 42/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 43/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 44/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 45/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 46/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 47/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 48/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 49/512\n",
      "183/183 [==============================] - 2s 12ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 50/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 51/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 52/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 53/512\n",
      "183/183 [==============================] - 2s 12ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 54/512\n",
      "183/183 [==============================] - 2s 12ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 55/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 56/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 57/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 58/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 59/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 60/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 61/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 62/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 63/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 64/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 65/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 66/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 67/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 68/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 69/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 70/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 71/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 72/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 73/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 74/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 75/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 76/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 77/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 78/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 79/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 80/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 81/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 82/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 83/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 84/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 85/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 86/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 87/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 88/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 89/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 90/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 91/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 92/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 93/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 94/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 95/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 96/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 97/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 98/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 99/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 100/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 101/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 102/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 103/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 104/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 105/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 106/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 107/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 108/512\n",
      "183/183 [==============================] - 2s 12ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 109/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 110/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 111/512\n",
      "183/183 [==============================] - 2s 12ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 112/512\n",
      "183/183 [==============================] - 2s 12ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 113/512\n",
      "183/183 [==============================] - 2s 12ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 114/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 115/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 116/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 117/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 118/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 119/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 120/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 121/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 122/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 123/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 124/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 125/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 126/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 127/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 128/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 129/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 130/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 131/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 132/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 133/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 134/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 135/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 136/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 137/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 138/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 139/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 140/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 141/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 142/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 143/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 144/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 145/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 146/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 147/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 148/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 149/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 150/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 151/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 152/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 153/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 154/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 155/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 156/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 157/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 158/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 159/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 160/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 161/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 162/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 163/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 164/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 165/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 166/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 167/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 168/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 169/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 170/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 171/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 172/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 173/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 174/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 175/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 176/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 177/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 178/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 179/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 180/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 181/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 182/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 183/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 184/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 185/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 186/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 187/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 188/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 189/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 190/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 191/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 192/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 193/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 194/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 195/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 196/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 197/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 198/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 199/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 200/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 201/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 202/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 203/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 204/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 205/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 206/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 207/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 208/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 209/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 210/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 211/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 212/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 213/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 214/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 215/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 216/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 217/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 218/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 219/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 220/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 221/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 222/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 223/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 224/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 225/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 226/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 227/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 228/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 229/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 230/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 231/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 232/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 233/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 234/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 235/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 236/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 237/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 238/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 239/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 240/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 241/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 242/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 243/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 244/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 245/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 246/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 247/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 248/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 249/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 250/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 251/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 252/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 253/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 254/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 255/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 256/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 257/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 258/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 259/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 260/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 261/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 262/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 263/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 264/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 265/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 266/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 267/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 268/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 269/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 270/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 271/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 272/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 273/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 274/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 275/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 276/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 277/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 278/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 279/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 280/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 281/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 282/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 283/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 284/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 285/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 286/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 287/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 288/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 289/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 290/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 291/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 292/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 293/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 294/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 295/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 296/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 297/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 298/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 299/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 300/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 301/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 302/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 303/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 304/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 305/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 306/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 307/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 308/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 309/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 310/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 311/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 312/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 313/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 314/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 315/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 316/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 317/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 318/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 319/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 320/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 321/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 322/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 323/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 324/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 325/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 326/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 327/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 328/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 329/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 330/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 331/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 332/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 333/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 334/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 335/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 336/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 337/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 338/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 339/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 340/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 341/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 342/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 343/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 344/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 345/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 346/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 347/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 348/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 349/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 350/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 351/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 352/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 353/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 354/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 355/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 356/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 357/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 358/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 359/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 360/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 361/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 362/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 363/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 364/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 365/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 366/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 367/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 368/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 369/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 370/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 371/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 372/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 373/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 374/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 375/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 376/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 377/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 378/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 379/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 380/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 381/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 382/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 383/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 384/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 385/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 386/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 387/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 388/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 389/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 390/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 391/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 392/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 393/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 394/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 395/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 396/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 397/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 398/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 399/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 400/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 401/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 402/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 403/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 404/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 405/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 406/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 407/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 408/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 409/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 410/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 411/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 412/512\n",
      "183/183 [==============================] - 2s 12ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 413/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 414/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 415/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 416/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 417/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 418/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 419/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 420/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 421/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 422/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 423/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 424/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 425/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 426/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 427/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 428/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 429/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 430/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 431/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 432/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 433/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 434/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 435/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 436/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 437/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 438/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 439/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 440/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 441/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 442/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 443/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 444/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 445/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 446/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 447/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 448/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 449/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 450/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 451/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 452/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 453/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 454/512\n",
      "183/183 [==============================] - 2s 12ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 455/512\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 456/512\n",
      "183/183 [==============================] - 2s 13ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 457/512\n",
      "183/183 [==============================] - 3s 17ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 458/512\n",
      "183/183 [==============================] - 3s 16ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 459/512\n",
      "183/183 [==============================] - 2s 13ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 460/512\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 461/512\n",
      "183/183 [==============================] - 3s 17ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 462/512\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 463/512\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 464/512\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 465/512\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.4406 - mse: 0.440 - 3s 14ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 466/512\n",
      "183/183 [==============================] - 3s 14ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 467/512\n",
      "183/183 [==============================] - 2s 13ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 468/512\n",
      "183/183 [==============================] - 2s 13ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 469/512\n",
      "183/183 [==============================] - 3s 15ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 470/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 471/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 472/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 473/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 474/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 475/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 476/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 477/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 478/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 479/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 480/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 481/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 482/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 483/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 484/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 485/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 486/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 487/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 488/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 489/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 490/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 491/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 492/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4396 - mse: 0.4396\n",
      "Epoch 493/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 494/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 495/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 496/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 497/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 498/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 499/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 500/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 501/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 502/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 503/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 504/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 505/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 506/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 507/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 508/512\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 509/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 510/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 511/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 512/512\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.4397 - mse: 0.4397\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,epochs = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "1ac52ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32884207543233024"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally check\n",
    "y_pred=model.predict(X_valid)\n",
    "np.mean((y_pred-y_valid)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "10ac7077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3741744653920835"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=model.predict(X_test)\n",
    "np.mean((y_pred-y_test)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a70f2fb",
   "metadata": {},
   "source": [
    "# Copying code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "09ee3c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_train='2016-01-01'\n",
    "date_validate='2018-01-01'\n",
    "\n",
    "DF=DF[['energy_generated','water_level','maximum_demand','influent_flow','date']]\n",
    "\n",
    "y_var='energy_generated'\n",
    "X_train=np.array(DF.drop(['date',y_var],axis=1)[DF.date<date_train])\n",
    "y_train=np.array(DF[y_var][DF.date<date_train])\n",
    "\n",
    "X_valid=np.array(DF.drop(['date',y_var],axis=1)[(date_train<=DF.date) & (DF.date<date_validate)])\n",
    "y_valid=np.array(DF[y_var][(date_train<=DF.date) & (DF.date<date_validate)])\n",
    "\n",
    "X_test=np.array(DF.drop(['date',y_var],axis=1)[DF.date>=date_validate])\n",
    "y_test=np.array(DF[y_var][DF.date>=date_validate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c797cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (len(X_train), 2, 1))\n",
    "X_valid = np.reshape(X_valid, (len(X_valid), 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "474c74f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_11 (LSTM)               (None, 5)                 140       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 176\n",
      "Trainable params: 176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.LSTM(5, activation='tanh',input_shape=[2,1]))\n",
    "model.add(layers.Dense(5))\n",
    "model.add(layers.Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "12083f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 1.7537 - mse: 1.7537 - val_loss: 1.0739 - val_mse: 1.0739\n",
      "Epoch 2/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.5917 - mse: 0.5917 - val_loss: 0.3434 - val_mse: 0.3434\n",
      "Epoch 3/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4401 - mse: 0.4401 - val_loss: 0.3321 - val_mse: 0.3321\n",
      "Epoch 4/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4401 - mse: 0.4401 - val_loss: 0.3256 - val_mse: 0.3256\n",
      "Epoch 5/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4399 - mse: 0.4399 - val_loss: 0.3193 - val_mse: 0.3193\n",
      "Epoch 6/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4402 - mse: 0.4402 - val_loss: 0.3298 - val_mse: 0.3298\n",
      "Epoch 7/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4400 - mse: 0.4400 - val_loss: 0.3225 - val_mse: 0.3225\n",
      "Epoch 8/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4402 - mse: 0.4402 - val_loss: 0.3290 - val_mse: 0.3290\n",
      "Epoch 9/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4400 - mse: 0.4400 - val_loss: 0.3265 - val_mse: 0.3265\n",
      "Epoch 10/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4399 - mse: 0.4399 - val_loss: 0.3306 - val_mse: 0.3306\n",
      "Epoch 11/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4400 - mse: 0.4400 - val_loss: 0.3295 - val_mse: 0.3295\n",
      "Epoch 12/200\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.4360 - mse: 0.436 - 0s 2ms/step - loss: 0.4400 - mse: 0.4400 - val_loss: 0.3273 - val_mse: 0.3273\n",
      "Epoch 13/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4399 - mse: 0.4399 - val_loss: 0.3274 - val_mse: 0.3274\n",
      "Epoch 14/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4399 - mse: 0.4399 - val_loss: 0.3336 - val_mse: 0.3336\n",
      "Epoch 15/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4403 - mse: 0.4403 - val_loss: 0.3283 - val_mse: 0.3283\n",
      "Epoch 16/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4399 - mse: 0.4399 - val_loss: 0.3338 - val_mse: 0.3338\n",
      "Epoch 17/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4400 - mse: 0.4400 - val_loss: 0.3302 - val_mse: 0.3302\n",
      "Epoch 18/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4400 - mse: 0.4400 - val_loss: 0.3287 - val_mse: 0.3287\n",
      "Epoch 19/200\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.4401 - mse: 0.4401 - val_loss: 0.3266 - val_mse: 0.3266\n",
      "Epoch 20/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4401 - mse: 0.4401 - val_loss: 0.3342 - val_mse: 0.3342\n",
      "Epoch 21/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4400 - mse: 0.4400 - val_loss: 0.3330 - val_mse: 0.3330\n",
      "Epoch 22/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4402 - mse: 0.4402 - val_loss: 0.3277 - val_mse: 0.3277\n",
      "Epoch 23/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4402 - mse: 0.4402 - val_loss: 0.3249 - val_mse: 0.3249\n",
      "Epoch 24/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4400 - mse: 0.4400 - val_loss: 0.3278 - val_mse: 0.3278\n",
      "Epoch 25/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4399 - mse: 0.4399 - val_loss: 0.3267 - val_mse: 0.3267\n",
      "Epoch 26/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4401 - mse: 0.4401 - val_loss: 0.3262 - val_mse: 0.3262\n",
      "Epoch 27/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4399 - mse: 0.4399 - val_loss: 0.3334 - val_mse: 0.3334\n",
      "Epoch 28/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4398 - mse: 0.4398 - val_loss: 0.3236 - val_mse: 0.3236\n",
      "Epoch 29/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4398 - mse: 0.4398 - val_loss: 0.3216 - val_mse: 0.3216\n",
      "Epoch 30/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4392 - mse: 0.4392 - val_loss: 0.3294 - val_mse: 0.3294\n",
      "Epoch 31/200\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.4397 - mse: 0.4397 - val_loss: 0.3441 - val_mse: 0.3441\n",
      "Epoch 32/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4403 - mse: 0.4403 - val_loss: 0.3332 - val_mse: 0.3332\n",
      "Epoch 33/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4402 - mse: 0.4402 - val_loss: 0.3295 - val_mse: 0.3295\n",
      "Epoch 34/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4401 - mse: 0.4401 - val_loss: 0.3297 - val_mse: 0.3297\n",
      "Epoch 35/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4400 - mse: 0.4400 - val_loss: 0.3348 - val_mse: 0.3348\n",
      "Epoch 36/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4399 - mse: 0.4399 - val_loss: 0.3207 - val_mse: 0.3207\n",
      "Epoch 37/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4401 - mse: 0.4401 - val_loss: 0.3273 - val_mse: 0.3273\n",
      "Epoch 38/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4399 - mse: 0.4399 - val_loss: 0.3247 - val_mse: 0.3247\n",
      "Epoch 39/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4401 - mse: 0.4401 - val_loss: 0.3340 - val_mse: 0.3340\n",
      "Epoch 40/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4399 - mse: 0.4399 - val_loss: 0.3296 - val_mse: 0.3296\n",
      "Epoch 41/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4399 - mse: 0.4399 - val_loss: 0.3352 - val_mse: 0.3352\n",
      "Epoch 42/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4399 - mse: 0.4399 - val_loss: 0.3353 - val_mse: 0.3353\n",
      "Epoch 43/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4397 - mse: 0.4397 - val_loss: 0.3427 - val_mse: 0.3427\n",
      "Epoch 44/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4399 - mse: 0.4399 - val_loss: 0.3186 - val_mse: 0.3186\n",
      "Epoch 45/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4401 - mse: 0.4401 - val_loss: 0.3329 - val_mse: 0.3329\n",
      "Epoch 46/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4400 - mse: 0.4400 - val_loss: 0.3277 - val_mse: 0.3277\n",
      "Epoch 47/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4400 - mse: 0.4400 - val_loss: 0.3339 - val_mse: 0.3339\n",
      "Epoch 48/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4401 - mse: 0.4401 - val_loss: 0.3350 - val_mse: 0.3350\n",
      "Epoch 49/200\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.4402 - mse: 0.4402 - val_loss: 0.3312 - val_mse: 0.3312\n",
      "Epoch 50/200\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.4399 - mse: 0.4399 - val_loss: 0.3340 - val_mse: 0.3340\n",
      "Epoch 51/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4400 - mse: 0.4400 - val_loss: 0.3280 - val_mse: 0.3280\n",
      "Epoch 52/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4398 - mse: 0.4398 - val_loss: 0.3188 - val_mse: 0.3188\n",
      "Epoch 53/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4400 - mse: 0.4400 - val_loss: 0.3327 - val_mse: 0.3327\n",
      "Epoch 54/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4397 - mse: 0.4397 - val_loss: 0.3319 - val_mse: 0.3319\n",
      "Epoch 55/200\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.4400 - mse: 0.4400 - val_loss: 0.3282 - val_mse: 0.3282\n",
      "Epoch 56/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4400 - mse: 0.4400 - val_loss: 0.3215 - val_mse: 0.3215\n",
      "Epoch 57/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4401 - mse: 0.4401 - val_loss: 0.3231 - val_mse: 0.3231\n",
      "Epoch 58/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4401 - mse: 0.4401 - val_loss: 0.3305 - val_mse: 0.3305\n",
      "Epoch 59/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4400 - mse: 0.4400 - val_loss: 0.3338 - val_mse: 0.3338\n",
      "Epoch 60/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4401 - mse: 0.4401 - val_loss: 0.3314 - val_mse: 0.3314\n",
      "Epoch 61/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4401 - mse: 0.4401 - val_loss: 0.3311 - val_mse: 0.3311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4399 - mse: 0.4399 - val_loss: 0.3263 - val_mse: 0.3263\n",
      "Epoch 63/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4397 - mse: 0.4397 - val_loss: 0.3320 - val_mse: 0.3320\n",
      "Epoch 64/200\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.4400 - mse: 0.4400 - val_loss: 0.3292 - val_mse: 0.3292\n",
      "Epoch 65/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4400 - mse: 0.4400 - val_loss: 0.3386 - val_mse: 0.3386\n",
      "Epoch 66/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4400 - mse: 0.4400 - val_loss: 0.3182 - val_mse: 0.3182\n",
      "Epoch 67/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4401 - mse: 0.4401 - val_loss: 0.3285 - val_mse: 0.3285\n",
      "Epoch 68/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4402 - mse: 0.4402 - val_loss: 0.3277 - val_mse: 0.3277\n",
      "Epoch 69/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4401 - mse: 0.4401 - val_loss: 0.3254 - val_mse: 0.3254\n",
      "Epoch 70/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4400 - mse: 0.4400 - val_loss: 0.3241 - val_mse: 0.3241\n",
      "Epoch 71/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4401 - mse: 0.4401 - val_loss: 0.3314 - val_mse: 0.3314\n",
      "Epoch 72/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4400 - mse: 0.4400 - val_loss: 0.3332 - val_mse: 0.3332\n",
      "Epoch 73/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4400 - mse: 0.4400 - val_loss: 0.3315 - val_mse: 0.3315\n",
      "Epoch 74/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4400 - mse: 0.4400 - val_loss: 0.3271 - val_mse: 0.3271\n",
      "Epoch 75/200\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.4398 - mse: 0.4398 - val_loss: 0.3298 - val_mse: 0.3298\n",
      "Epoch 76/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4397 - mse: 0.4397 - val_loss: 0.3355 - val_mse: 0.3355\n",
      "Epoch 77/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4400 - mse: 0.4400 - val_loss: 0.3268 - val_mse: 0.3268\n",
      "Epoch 78/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4401 - mse: 0.4401 - val_loss: 0.3275 - val_mse: 0.3275\n",
      "Epoch 79/200\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4399 - mse: 0.4399 - val_loss: 0.3229 - val_mse: 0.3229\n",
      "Epoch 80/200\n",
      "166/183 [==========================>...] - ETA: 0s - loss: 0.4375 - mse: 0.4375"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-f6c29a8d4d8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rmsprop'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1121\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[1;32m-> 1123\u001b[1;33m           val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m   1124\u001b[0m               \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1125\u001b[0m               \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1377\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TraceContext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',loss='mse', metrics=['mse'])\n",
    "history = model.fit(X_train,y_train,epochs = 200,validation_data=(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e1abd0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3309296073608943"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally check\n",
    "y_valid=np.reshape(y_valid,(len(y_valid),1))\n",
    "y_pred=model.predict(X_valid)\n",
    "np.mean((y_pred-y_valid)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2f85e3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_pred,color='blue')\n",
    "plt.plot(y_valid,color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06ec994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c0615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
